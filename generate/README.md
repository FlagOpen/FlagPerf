### éƒ¨ç½²è¯´æ˜

#### ä»£ç ç›®å½•è¯´æ˜
```
â”œâ”€â”€ README.md
â”œâ”€â”€ main.py #ç”¨æˆ·è°ƒç”¨æ¥å£
â”œâ”€â”€ utils #ç”¨äºåˆ†æçš„å·¥å…·
|    â”œâ”€â”€ analyze.py #åˆ†æé…ç½®ä¿¡æ¯
|    â”œâ”€â”€ result_show.py #åˆ†æç»“æœä¿¡æ¯
|â€”â€” Throughput #ç”¨äºè§‚æµ‹ååé‡ä¿¡æ¯
|    â”œâ”€â”€ vendor #å‚å•†é…ç½®
|          â”œâ”€â”€ engine #æ¨ç†å¼•æ“
|â€”â€” TTFT #ç”¨äºè§‚æµ‹é¦–å­—å»¶è¿Ÿ
|    â”œâ”€â”€ vendor #å‚å•†é…ç½®
|          â”œâ”€â”€ engine #æ¨ç†å¼•æ“
|â€”â€” TASK #é…ç½®ä»»åŠ¡ä¿¡æ¯
|    â”œâ”€â”€ vendor #å‚å•†é…ç½®
|          â”œâ”€â”€ engine #æ¨ç†å¼•æ“
|                 |â€”â€” GPUConfig.yaml #ç¡¬ä»¶ä¿¡æ¯é…ç½®
|â€”â€” host.yaml #è·¯å¾„ä¿¡æ¯é…ç½®

```
#### æ•°æ®é›†
1. æœ¬æ¬¡é‡‡ç”¨çš„æ˜¯å¼€æºæ•°æ®é›†XSumï¼Œè¯¥æ•°æ®é›†ä¾§é‡äºæ¨¡å‹å¯¹æ–‡æœ¬æ‘˜è¦çš„ç”Ÿæˆï¼Œåé‡äºæ¨¡å‹æ¨ç†ã€‚
2. æ•°æ®é›†ä¸‹è½½åœ°å€ https://huggingface.co/datasets/knkarthick/xsum/tree/mainï¼Œé‡‡ç”¨çš„æ˜¯è¯¥ä»“åº“ä¸­å…¨éƒ¨æ•°æ®ä½œä¸ºæµ‹è¯•é›†ã€‚https://github.com/EdinburghNLP/XSum/tree/master/ æºæ•°æ®æ˜¯ä»¥.summarå½¢å¼å­˜å‚¨ï¼Œå¦‚æœä½¿ç”¨æºæ•°æ®é›†åˆ™éœ€è¦æŒ‰ç…§å‰é“¾æ¥çš„æ•°æ®é›†å½¢å¼è¿›è¡Œè½¬æ¢å¹¶ä»¥csvæ–‡ä»¶çš„å½¢å¼å­˜å‚¨
3. æ•°æ®é›†è¯„æµ‹æ–¹å¼ï¼šé‡‡ç”¨ROUGEåˆ†æ•°å¯¹æ¨ç†ç»“æœè¿›è¡Œè¯„æµ‹ï¼ŒåŒåŸè®ºæ–‡https://arxiv.org/abs/1808.08745 çš„æµ‹é‡æ–¹å¼ä¿æŒä¸€è‡´ã€‚
#### é…ç½®æ–‡ä»¶è¯´æ˜
1. host.yaml
    1. model_path: æ¨¡å‹å­˜æ”¾è·¯å¾„
    2. data_path: æ•°æ®é›†å­˜æ”¾è·¯å¾„
    3. log_path: æ¨ç†ç»“æœå­˜æ”¾è·¯å¾„
    4. engine: æ¨ç†æ¡†æ¶(æ”¯æŒæ‰©å±•ï¼Œç°ç‰ˆæœ¬æ”¯æŒvllmä»¥åŠhuggingfaceæ¡†æ¶ä¸‹çš„æ¨ç†)
    5. gpu_nums:é‡‡ç”¨çš„gpuæ•°é‡
    6. vendor: å‚å•†åç§°
    7. config_path: é…ç½®æ–‡ä»¶å­˜æ”¾è·¯å¾„
2. vendor/engine/task.yaml
    1. GPU_NAME: GPUåç§°
    2. GPU_FPxx: åœ¨xxç²¾åº¦ä¸‹GPUçš„ç†è®ºå³°å€¼FLOPs(å•ä½ä¸ºTFLOPs)
    3. task_nums: ä»»åŠ¡æ•°é‡

#### è¿è¡Œæ–¹å¼
å‚å•†ä¿®æ”¹å®Œtask.yamlå’Œhost.yamlæ–‡ä»¶åï¼Œè°ƒç”¨python main.py å³å¯è¿›è¡Œè¯„æµ‹ï¼Œè¯„æµ‹ç»“æœä¼šæ˜¾ç¤ºåœ¨æ§åˆ¶å°ä»¥åŠ/log/engine/ä¹‹ä¸­çš„logæ–‡ä»¶
#### è¿è¡Œç»“æœ
è¿è¡Œç»“æœä¼šæ˜¾ç¤ºåœ¨å±å¹•ä¸­ï¼Œå¦‚ï¼š
2024-09-19 05:56:48.920 | INFO     | __main__:<module>:37 - TTFT:0.7435413458806579
2024-09-19 05:56:48.920 | INFO     | __main__:<module>:38 - Throughput:21.241251614991388
2024-09-19 05:56:48.920 | INFO     | __main__:<module>:39 - Tps:640.5137188603395
2024-09-19 05:56:48.920 | INFO     | __main__:<module>:40 - Time:193.57586941402406
2024-09-19 05:56:48.920 | INFO     | __main__:<module>:41 - MFU:0.0036210430230075204
2024-09-19 05:56:48.921 | INFO     | __main__:<module>:42 - ROUGE1:0.10941584374196911
2024-09-19 05:56:48.921 | INFO     | __main__:<module>:43 - ROUGE2:0.002265168149848077
è¿è¡Œè¿‡ç¨‹ä¸­çš„è¾“å‡ºä¼šè®°å½•åœ¨output.txtæ–‡ä»¶ä¸­
å¦‚ï¼š2024-09-19 06:22:31,864	INFO worker.py:1783 -- Started a local Ray instance.
INFO 09-19 06:22:41 llm_engine.py:72] Initializing an LLM engine with config: model='/raid/llama3_infer/llama3_70b_hf', tokenizer='/raid/llama3_infer/llama3_70b_hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=8, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, seed=0)
INFO 09-19 06:23:40 llm_engine.py:322] # GPU blocks: 23500, # CPU blocks: 6553
INFO 09-19 06:23:44 model_runner.py:632] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 09-19 06:23:44 model_runner.py:636] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerVllm pid=83185)[0m INFO 09-19 06:23:44 model_runner.py:632] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerVllm pid=83185)[0m INFO 09-19 06:23:44 model_runner.py:636] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 09-19 06:23:55 custom_all_reduce.py:199] Registering 5635 cuda graph addresses
[36m(RayWorkerVllm pid=83185)[0m INFO 09-19 06:23:55 custom_all_reduce.py:199] Registering 5635 cuda graph addresses
[36m(RayWorkerVllm pid=84278)[0m INFO 09-19 06:23:44 model_runner.py:632] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(RayWorkerVllm pid=84278)[0m INFO 09-19 06:23:44 model_runner.py:636] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[32m [repeated 6x across cluster][0m
INFO 09-19 06:23:55 model_runner.py:698] Graph capturing finished in 11 secs.
[36m(RayWorkerVllm pid=83185)[0m INFO 09-19 06:23:55 model_runner.py:698] Graph capturing finished in 11 secs.

Processed prompts:   0%|          | 0/5000 [00:00<?, ?it/s]
Processed prompts:   0%|          | 1/5000 [00:00<11:13,  7.42it/s]
Processed prompts:   0%|          | 3/5000 [00:01<36:56,  2.25it/s]
Processed prompts:   0%|          | 4/5000 [00:01<27:28,  3.03it/s]
Processed prompts:   0%|          | 6/5000 [00:01<16:27,  5.06it/s]
Processed prompts:   0%|          | 7/5000 [00:01<17:00,  4.89it/s]
Processed prompts:   0%|          | 12/5000 [00:01<07:51, 10.57it/s]

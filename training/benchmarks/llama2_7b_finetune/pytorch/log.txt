root@3860d230382f:/home/wuwenli/Llama2-Chinese/FlagPerf# python3 /home/wuwenli/Llama2-Chinese/FlagPerf/training/run_benchmarks/pytorch/start_pytorch_task.py  --vendor nvidia --case_name llama2_7b_finetune:pytorch_1.8:A100:1:1:1 --model_name llama2_7b_finetune --train_script run_pretraining.py --nnodes 1 --nproc 1 --hosts 10.1.2.155 --hosts_ports 2222 --data_dir /home/wuwenli/Llama2-Chinese/FlagPerf/training --log_dir /home/wuwenli/Llama2-Chinese/FlagPerf/training/result/run20231025103324 --log_level debug --extern_config_file config_A100x1x1.py --enable_extern_config  --master_port 29501 --round 1 --visible_dev_env CUDA_VISIBLE_DEVICES  --master_addr 10.1.2.155 --node_rank 0 --host_addr 10.1.2.155
2023-11-01 10:38:47,378 [INFO]  [start_pytorch_task.py,185]Start task with command: /opt/conda/bin/python3 -u /home/wuwenli/Llama2-Chinese/FlagPerf/training/benchmarks/llama2_7b_finetune/pytorch/run_pretraining.py  --extern_config_dir /home/wuwenli/Llama2-Chinese/FlagPerf/training/nvidia/llama2_7b_finetune-pytorch/config --extern_config_file config_A100x1x1.py --vendor nvidia --data_dir /home/wuwenli/Llama2-Chinese/FlagPerf/training --enable_extern_config --extern_module_dir /home/wuwenli/Llama2-Chinese/FlagPerf/training/nvidia/llama2_7b_finetune-pytorch/extern 2>&1 | tee /home/wuwenli/Llama2-Chinese/FlagPerf/training/result/run20231025103324/llama2_7b_finetune:pytorch_1.8:A100:1:1:1/round1/10.1.2.155_noderank0/rank0.out.log
2023-11-01 10:38:47,378 [DEBUG] [start_pytorch_task.py,186]----------- Process envs -----------
2023-11-01 10:38:47,378 [DEBUG] [start_pytorch_task.py,188]NV_LIBCUBLAS_VERSION:11.10.1.25-1
2023-11-01 10:38:47,379 [DEBUG] [start_pytorch_task.py,188]NVIDIA_VISIBLE_DEVICES:all
2023-11-01 10:38:47,379 [DEBUG] [start_pytorch_task.py,188]NV_NVML_DEV_VERSION:11.7.50-1
2023-11-01 10:38:47,379 [DEBUG] [start_pytorch_task.py,188]NV_CUDNN_PACKAGE_NAME:libcudnn8
2023-11-01 10:38:47,379 [DEBUG] [start_pytorch_task.py,188]NV_LIBNCCL_DEV_PACKAGE:libnccl-dev=2.13.4-1+cuda11.7
2023-11-01 10:38:47,379 [DEBUG] [start_pytorch_task.py,188]NV_LIBNCCL_DEV_PACKAGE_VERSION:2.13.4-1
2023-11-01 10:38:47,379 [DEBUG] [start_pytorch_task.py,188]HOSTNAME:3860d230382f
2023-11-01 10:38:47,379 [DEBUG] [start_pytorch_task.py,188]NCCL_P2P_DISABLE:0
2023-11-01 10:38:47,379 [DEBUG] [start_pytorch_task.py,188]NVIDIA_REQUIRE_CUDA:cuda>=11.7 brand=tesla,driver>=450,driver<451 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=510,driver<511 brand=unknown,driver>=510,driver<511 brand=nvidia,driver>=510,driver<511 brand=nvidiartx,driver>=510,driver<511 brand=quadro,driver>=510,driver<511 brand=quadrortx,driver>=510,driver<511 brand=titan,driver>=510,driver<511 brand=titanrtx,driver>=510,driver<511 brand=geforce,driver>=510,driver<511 brand=geforcertx,driver>=510,driver<511
2023-11-01 10:38:47,379 [DEBUG] [start_pytorch_task.py,188]NV_LIBCUBLAS_DEV_PACKAGE:libcublas-dev-11-7=11.10.1.25-1
2023-11-01 10:38:47,379 [DEBUG] [start_pytorch_task.py,188]NV_NVTX_VERSION:11.7.50-1
2023-11-01 10:38:47,379 [DEBUG] [start_pytorch_task.py,188]NV_CUDA_CUDART_DEV_VERSION:11.7.60-1
2023-11-01 10:38:47,379 [DEBUG] [start_pytorch_task.py,188]NV_LIBCUSPARSE_VERSION:11.7.3.50-1
2023-11-01 10:38:47,379 [DEBUG] [start_pytorch_task.py,188]NV_LIBNPP_VERSION:11.7.3.21-1
2023-11-01 10:38:47,379 [DEBUG] [start_pytorch_task.py,188]NCCL_VERSION:2.13.4-1
2023-11-01 10:38:47,379 [DEBUG] [start_pytorch_task.py,188]PWD:/home/wuwenli/Llama2-Chinese/FlagPerf
2023-11-01 10:38:47,379 [DEBUG] [start_pytorch_task.py,188]NV_CUDNN_PACKAGE:libcudnn8=8.5.0.96-1+cuda11.7
2023-11-01 10:38:47,379 [DEBUG] [start_pytorch_task.py,188]NVIDIA_DRIVER_CAPABILITIES:compute,utility
2023-11-01 10:38:47,379 [DEBUG] [start_pytorch_task.py,188]NV_NVPROF_DEV_PACKAGE:cuda-nvprof-11-7=11.7.50-1
2023-11-01 10:38:47,379 [DEBUG] [start_pytorch_task.py,188]NV_LIBNPP_PACKAGE:libnpp-11-7=11.7.3.21-1
2023-11-01 10:38:47,379 [DEBUG] [start_pytorch_task.py,188]NCCL_DEBUG:INFO
2023-11-01 10:38:47,380 [DEBUG] [start_pytorch_task.py,188]NV_LIBNCCL_DEV_PACKAGE_NAME:libnccl-dev
2023-11-01 10:38:47,380 [DEBUG] [start_pytorch_task.py,188]NV_LIBCUBLAS_DEV_VERSION:11.10.1.25-1
2023-11-01 10:38:47,380 [DEBUG] [start_pytorch_task.py,188]NCCL_IB_HCA:mlx5_2,mlx5_5
2023-11-01 10:38:47,380 [DEBUG] [start_pytorch_task.py,188]NVIDIA_PRODUCT_NAME:CUDA
2023-11-01 10:38:47,380 [DEBUG] [start_pytorch_task.py,188]NV_LIBCUBLAS_DEV_PACKAGE_NAME:libcublas-dev-11-7
2023-11-01 10:38:47,380 [DEBUG] [start_pytorch_task.py,188]NV_CUDA_CUDART_VERSION:11.7.60-1
2023-11-01 10:38:47,380 [DEBUG] [start_pytorch_task.py,188]HOME:/root
2023-11-01 10:38:47,380 [DEBUG] [start_pytorch_task.py,188]LS_COLORS:rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:
2023-11-01 10:38:47,380 [DEBUG] [start_pytorch_task.py,188]NVIDIA_CUDA_END_OF_LIFE:1
2023-11-01 10:38:47,380 [DEBUG] [start_pytorch_task.py,188]CUDA_VERSION:11.7.0
2023-11-01 10:38:47,380 [DEBUG] [start_pytorch_task.py,188]NV_LIBCUBLAS_PACKAGE:libcublas-11-7=11.10.1.25-1
2023-11-01 10:38:47,380 [DEBUG] [start_pytorch_task.py,188]PYTORCH_VERSION:2.0.1
2023-11-01 10:38:47,380 [DEBUG] [start_pytorch_task.py,188]https_proxy:http://10.1.0.34:7890
2023-11-01 10:38:47,380 [DEBUG] [start_pytorch_task.py,188]NV_LIBNPP_DEV_PACKAGE:libnpp-dev-11-7=11.7.3.21-1
2023-11-01 10:38:47,380 [DEBUG] [start_pytorch_task.py,188]NV_LIBCUBLAS_PACKAGE_NAME:libcublas-11-7
2023-11-01 10:38:47,380 [DEBUG] [start_pytorch_task.py,188]NV_LIBNPP_DEV_VERSION:11.7.3.21-1
2023-11-01 10:38:47,380 [DEBUG] [start_pytorch_task.py,188]CUDA_VISIBLE_DEVICES:0
2023-11-01 10:38:47,380 [DEBUG] [start_pytorch_task.py,188]LESSCLOSE:/usr/bin/lesspipe %s %s
2023-11-01 10:38:47,380 [DEBUG] [start_pytorch_task.py,188]PYTHONPATH:/home/wuwenli/Llama2-Chinese/llama-recipes:/home/wuwenli/Llama2-Chinese/llama-recipes:
2023-11-01 10:38:47,380 [DEBUG] [start_pytorch_task.py,188]TERM:xterm
2023-11-01 10:38:47,380 [DEBUG] [start_pytorch_task.py,188]NV_LIBCUSPARSE_DEV_VERSION:11.7.3.50-1
2023-11-01 10:38:47,381 [DEBUG] [start_pytorch_task.py,188]LESSOPEN:| /usr/bin/lesspipe %s
2023-11-01 10:38:47,381 [DEBUG] [start_pytorch_task.py,188]LIBRARY_PATH:/usr/local/cuda/lib64/stubs
2023-11-01 10:38:47,381 [DEBUG] [start_pytorch_task.py,188]NV_CUDNN_VERSION:8.5.0.96
2023-11-01 10:38:47,381 [DEBUG] [start_pytorch_task.py,188]SHLVL:1
2023-11-01 10:38:47,381 [DEBUG] [start_pytorch_task.py,188]NV_CUDA_LIB_VERSION:11.7.0-1
2023-11-01 10:38:47,381 [DEBUG] [start_pytorch_task.py,188]NVARCH:x86_64
2023-11-01 10:38:47,381 [DEBUG] [start_pytorch_task.py,188]NV_CUDNN_PACKAGE_DEV:libcudnn8-dev=8.5.0.96-1+cuda11.7
2023-11-01 10:38:47,381 [DEBUG] [start_pytorch_task.py,188]http_proxy:http://10.1.0.34:7890
2023-11-01 10:38:47,381 [DEBUG] [start_pytorch_task.py,188]NV_CUDA_COMPAT_PACKAGE:cuda-compat-11-7
2023-11-01 10:38:47,381 [DEBUG] [start_pytorch_task.py,188]NV_LIBNCCL_PACKAGE:libnccl2=2.13.4-1+cuda11.7
2023-11-01 10:38:47,381 [DEBUG] [start_pytorch_task.py,188]LD_LIBRARY_PATH:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2023-11-01 10:38:47,381 [DEBUG] [start_pytorch_task.py,188]NV_NVPROF_VERSION:11.7.50-1
2023-11-01 10:38:47,381 [DEBUG] [start_pytorch_task.py,188]all_proxy:socks5h://10.1.0.34:7891
2023-11-01 10:38:47,381 [DEBUG] [start_pytorch_task.py,188]PATH:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
2023-11-01 10:38:47,381 [DEBUG] [start_pytorch_task.py,188]NV_LIBNCCL_PACKAGE_NAME:libnccl2
2023-11-01 10:38:47,381 [DEBUG] [start_pytorch_task.py,188]NV_LIBNCCL_PACKAGE_VERSION:2.13.4-1
2023-11-01 10:38:47,381 [DEBUG] [start_pytorch_task.py,188]OLDPWD:/home/wuwenli/Llama2-Chinese/llama-recipes
2023-11-01 10:38:47,381 [DEBUG] [start_pytorch_task.py,188]_:/opt/conda/bin/python3
2023-11-01 10:38:47,381 [DEBUG] [start_pytorch_task.py,188]LC_CTYPE:C.UTF-8
2023-11-01 10:38:47,381 [DEBUG] [start_pytorch_task.py,188]MASTER_ADDR:10.1.2.155
2023-11-01 10:38:47,381 [DEBUG] [start_pytorch_task.py,188]MASTER_PORT:29501
2023-11-01 10:38:47,382 [DEBUG] [start_pytorch_task.py,188]WORLD_SIZE:1
2023-11-01 10:38:47,382 [DEBUG] [start_pytorch_task.py,188]NODE_RANK:0
2023-11-01 10:38:47,382 [DEBUG] [start_pytorch_task.py,188]RANK:0
2023-11-01 10:38:47,382 [DEBUG] [start_pytorch_task.py,188]LOCAL_RANK:0
2023-11-01 10:38:47,382 [DEBUG] [start_pytorch_task.py,189]start command: /opt/conda/bin/python3 -u /home/wuwenli/Llama2-Chinese/FlagPerf/training/benchmarks/llama2_7b_finetune/pytorch/run_pretraining.py  --extern_config_dir /home/wuwenli/Llama2-Chinese/FlagPerf/training/nvidia/llama2_7b_finetune-pytorch/config --extern_config_file config_A100x1x1.py --vendor nvidia --data_dir /home/wuwenli/Llama2-Chinese/FlagPerf/training --enable_extern_config --extern_module_dir /home/wuwenli/Llama2-Chinese/FlagPerf/training/nvidia/llama2_7b_finetune-pytorch/extern 2>&1 | tee /home/wuwenli/Llama2-Chinese/FlagPerf/training/result/run20231025103324/llama2_7b_finetune:pytorch_1.8:A100:1:1:1/round1/10.1.2.155_noderank0/rank0.out.log

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so
/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}
  warn(msg)
/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//10.1.0.34'), PosixPath('socks5h'), PosixPath('7891')}
  warn(msg)
/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('//10.1.0.34'), PosixPath('7890')}
  warn(msg)
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.
Either way, this might cause trouble in the future:
If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.
  warn(msg)
CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 8.0
CUDA SETUP: Detected CUDA version 117
CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...
[2023-11-01 10:38:50,874] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
SET [Unknown or immutable] CONFIG batch_size_training = 2
SET [Unknown or immutable] CONFIG target_MMLU = 0.35
SET [Unknown or immutable] CONFIG num_epochs = 3
{'__name__': '__main__', '__doc__': None, '__package__': None, '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7fc3f3df9330>, '__spec__': None, '__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__file__': '/home/wuwenli/Llama2-Chinese/FlagPerf/training/benchmarks/llama2_7b_finetune/pytorch/run_pretraining.py', '__cached__': None, 'os': <module 'os' from '/opt/conda/lib/python3.10/os.py'>, 'time': <module 'time' (built-in)>, 'logger': <loguru.logger handlers=[(id=0, level=10, sink=<stderr>)]>, 'sys': <module 'sys' (built-in)>, 'tqdm': <class 'tqdm.std.tqdm'>, 'nullcontext': <class 'contextlib.nullcontext'>, 'torch': <module 'torch' from '/opt/conda/lib/python3.10/site-packages/torch/__init__.py'>, 'dist': <module 'torch.distributed' from '/opt/conda/lib/python3.10/site-packages/torch/distributed/__init__.py'>, 'DataLoader': <class 'torch.utils.data.dataloader.DataLoader'>, 'DistributedSampler': <class 'torch.utils.data.distributed.DistributedSampler'>, 'default_data_collator': <function default_data_collator at 0x7fc355ff51b0>, 'CURR_PATH': '/home/wuwenli/Llama2-Chinese/FlagPerf/training/benchmarks/llama2_7b_finetune/pytorch', 'InitHelper': <class 'driver.helper.InitHelper'>, 'dist_pytorch': <module 'driver.dist_pytorch' from '/home/wuwenli/Llama2-Chinese/FlagPerf/training/benchmarks/driver/dist_pytorch.py'>, 'MemoryTrace': <class 'utils.memory_utils.MemoryTrace'>, 'evaluate_MMLU': <function evaluate_MMLU at 0x7fc2b05d1510>, 'get_llama_model': <function get_llama_model at 0x7fc1e8330b80>, 'get_llama_dataset': <function get_llama_dataset at 0x7fc2b05d16c0>, 'create_scheduler': <function create_scheduler at 0x7fc2a02e0c10>, 'create_optimizer': <function create_optimizer at 0x7fc1e8330ca0>, 'config': <module 'config' from '/home/wuwenli/Llama2-Chinese/FlagPerf/training/benchmarks/llama2_7b_finetune/pytorch/config/__init__.py'>, 'train': <function train at 0x7fc356104d30>, 'evaluation': <function evaluation at 0x7fc1e8330dc0>, 'main': <function main at 0x7fc1e8330e50>} remap by {}
{'init_helper': <driver.helper.InitHelper object at 0x7fc3f3ceb160>} remap by {}
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.91s/it]
trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199
You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
Training Epoch: 1/3, step 3109/3110 completed (loss: 2.09281587600708): 100%|██████████| 3110/3110 [1:25:51<00:00,  1.66s/it]
2023-11-01 12:06:13.389 | INFO     | __main__:train:101 - Max CUDA memory allocated was 36 GB
2023-11-01 12:06:13.389 | INFO     | __main__:train:102 - Max CUDA memory reserved was 36 GB
2023-11-01 12:06:13.389 | INFO     | __main__:train:103 - Peak active CUDA memory was 36 GB
2023-11-01 12:06:13.389 | INFO     | __main__:train:104 - Cuda Malloc retires : 0
2023-11-01 12:06:13.389 | INFO     | __main__:train:105 - CPU Total Peak Memory consumed during the train (max): 3 GB
2023-11-01 12:06:13.389 | INFO     | __main__:train:126 - Epoch 1: train_perplexity=5.7017, train_epoch_loss=1.7408, epoch time 5151.660983685404s
Training Epoch: 2/3, step 3109/3110 completed (loss: 2.0226659774780273): 100%|██████████| 3110/3110 [1:25:53<00:00,  1.66s/it]
2023-11-01 13:32:06.839 | INFO     | __main__:train:101 - Max CUDA memory allocated was 36 GB
2023-11-01 13:32:06.840 | INFO     | __main__:train:102 - Max CUDA memory reserved was 36 GB
2023-11-01 13:32:06.840 | INFO     | __main__:train:103 - Peak active CUDA memory was 36 GB
2023-11-01 13:32:06.840 | INFO     | __main__:train:104 - Cuda Malloc retires : 0
2023-11-01 13:32:06.840 | INFO     | __main__:train:105 - CPU Total Peak Memory consumed during the train (max): 4 GB
2023-11-01 13:32:06.840 | INFO     | __main__:train:126 - Epoch 2: train_perplexity=5.4170, train_epoch_loss=1.6895, epoch time 5153.449492134154s
Training Epoch: 3/3, step 3109/3110 completed (loss: 1.9543269872665405): 100%|██████████| 3110/3110 [1:25:50<00:00,  1.66s/it]
2023-11-01 14:57:57.840 | INFO     | __main__:train:101 - Max CUDA memory allocated was 36 GB
2023-11-01 14:57:57.840 | INFO     | __main__:train:102 - Max CUDA memory reserved was 36 GB
2023-11-01 14:57:57.840 | INFO     | __main__:train:103 - Peak active CUDA memory was 36 GB
2023-11-01 14:57:57.840 | INFO     | __main__:train:104 - Cuda Malloc retires : 0
2023-11-01 14:57:57.840 | INFO     | __main__:train:105 - CPU Total Peak Memory consumed during the train (max): 4 GB
2023-11-01 14:57:57.840 | INFO     | __main__:train:126 - Epoch 3: train_perplexity=5.1936, train_epoch_loss=1.6474, epoch time 5150.999320749193s
2023-11-01 14:57:57.901 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot abstract_algebra
2023-11-01 14:57:58.134 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot anatomy
2023-11-01 14:57:58.428 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot astronomy
2023-11-01 14:57:58.947 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot business_ethics
2023-11-01 14:57:59.283 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot clinical_knowledge
2023-11-01 14:57:59.916 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot college_biology
2023-11-01 14:58:00.321 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot college_chemistry
2023-11-01 14:58:00.587 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot college_computer_science
2023-11-01 14:58:01.017 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot college_mathematics
2023-11-01 14:58:01.303 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot college_medicine
2023-11-01 14:58:01.843 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot college_physics
2023-11-01 14:58:02.113 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot computer_security
2023-11-01 14:58:02.346 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot conceptual_physics
2023-11-01 14:58:02.839 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot econometrics
2023-11-01 14:58:03.190 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot electrical_engineering
2023-11-01 14:58:03.514 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot elementary_mathematics
2023-11-01 14:58:04.526 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot formal_logic
2023-11-01 14:58:04.926 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot global_facts
2023-11-01 14:58:05.164 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot high_school_biology
2023-11-01 14:58:06.084 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot high_school_chemistry
2023-11-01 14:58:06.599 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot high_school_computer_science
2023-11-01 14:58:07.061 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot high_school_european_history
2023-11-01 14:58:10.334 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot high_school_geography
2023-11-01 14:58:10.827 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot high_school_government_and_politics
2023-11-01 14:58:11.416 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot high_school_macroeconomics
2023-11-01 14:58:12.391 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot high_school_mathematics
2023-11-01 14:58:13.076 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot high_school_microeconomics
2023-11-01 14:58:13.679 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot high_school_physics
2023-11-01 14:58:14.124 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot high_school_psychology
2023-11-01 14:58:15.816 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot high_school_statistics
2023-11-01 14:58:16.708 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot high_school_us_history
2023-11-01 14:58:19.714 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot high_school_world_history
2023-11-01 14:58:21.718 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot human_aging
2023-11-01 14:58:22.194 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot human_sexuality
2023-11-01 14:58:22.489 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot international_law
2023-11-01 14:58:22.946 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot jurisprudence
2023-11-01 14:58:23.219 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot logical_fallacies
2023-11-01 14:58:23.681 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot machine_learning
2023-11-01 14:58:24.083 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot management
2023-11-01 14:58:24.290 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot marketing
2023-11-01 14:58:24.903 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot medical_genetics
2023-11-01 14:58:25.127 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot miscellaneous
2023-11-01 14:58:26.619 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot moral_disputes
2023-11-01 14:58:27.651 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot moral_scenarios
2023-11-01 14:58:30.834 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot nutrition
2023-11-01 14:58:31.994 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot philosophy
2023-11-01 14:58:32.673 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot prehistory
2023-11-01 14:58:33.860 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot professional_accounting
2023-11-01 14:58:35.071 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot professional_law
2023-11-01 14:58:55.309 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot professional_medicine
2023-11-01 14:58:57.305 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot professional_psychology
2023-11-01 14:58:59.935 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot public_relations
2023-11-01 14:59:00.282 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot security_studies
2023-11-01 14:59:02.617 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot sociology
2023-11-01 14:59:03.303 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot us_foreign_policy
2023-11-01 14:59:03.636 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot virology
2023-11-01 14:59:04.072 | DEBUG    | dataset.dataloader_mmlu:__init__:105 - Loading 5-shot world_religions
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2023-11-01 14:59:04.493 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 0 / 14042
2023-11-01 14:59:38.325 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 100 / 14042
2023-11-01 15:00:14.475 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 200 / 14042
2023-11-01 15:01:04.699 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 300 / 14042
2023-11-01 15:02:02.586 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 400 / 14042
2023-11-01 15:02:57.627 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 500 / 14042
2023-11-01 15:03:39.896 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 600 / 14042
2023-11-01 15:04:22.174 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 700 / 14042
2023-11-01 15:05:07.133 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 800 / 14042
2023-11-01 15:05:55.907 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 900 / 14042
2023-11-01 15:06:49.118 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 1000 / 14042
2023-11-01 15:08:05.269 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 1100 / 14042
2023-11-01 15:08:59.095 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 1200 / 14042
2023-11-01 15:09:52.823 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 1300 / 14042
2023-11-01 15:10:42.705 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 1400 / 14042
2023-11-01 15:11:28.182 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 1500 / 14042
2023-11-01 15:12:02.878 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 1600 / 14042
2023-11-01 15:12:31.993 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 1700 / 14042
2023-11-01 15:13:01.046 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 1800 / 14042
2023-11-01 15:13:57.218 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 1900 / 14042
2023-11-01 15:14:43.542 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 2000 / 14042
2023-11-01 15:15:29.285 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 2100 / 14042
2023-11-01 15:16:20.395 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 2200 / 14042
2023-11-01 15:17:11.120 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 2300 / 14042
2023-11-01 15:18:02.095 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 2400 / 14042
2023-11-01 15:18:56.541 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 2500 / 14042
2023-11-01 15:19:49.755 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 2600 / 14042
2023-11-01 15:20:34.391 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 2700 / 14042
2023-11-01 15:21:24.867 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 2800 / 14042
2023-11-01 15:22:15.375 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 2900 / 14042
2023-11-01 15:23:05.460 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 3000 / 14042
2023-11-01 15:23:55.064 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 3100 / 14042
2023-11-01 15:24:51.203 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 3200 / 14042
2023-11-01 15:26:53.047 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 3300 / 14042
2023-11-01 15:31:44.481 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 3400 / 14042
2023-11-01 15:34:22.195 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 3500 / 14042
2023-11-01 15:35:00.980 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 3600 / 14042
2023-11-01 15:35:44.546 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 3700 / 14042
2023-11-01 15:36:31.160 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 3800 / 14042
2023-11-01 15:37:13.295 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 3900 / 14042
2023-11-01 15:37:52.836 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 4000 / 14042
2023-11-01 15:38:32.143 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 4100 / 14042
2023-11-01 15:39:11.604 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 4200 / 14042
2023-11-01 15:39:58.082 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 4300 / 14042
2023-11-01 15:40:47.473 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 4400 / 14042
2023-11-01 15:41:36.883 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 4500 / 14042
2023-11-01 15:42:16.349 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 4600 / 14042
2023-11-01 15:42:55.916 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 4700 / 14042
2023-11-01 15:43:43.798 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 4800 / 14042
2023-11-01 15:44:35.225 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 4900 / 14042
2023-11-01 15:45:25.339 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 5000 / 14042
2023-11-01 15:46:14.890 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 5100 / 14042
2023-11-01 15:47:04.774 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 5200 / 14042
2023-11-01 15:47:54.402 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 5300 / 14042
2023-11-01 15:48:44.063 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 5400 / 14042
2023-11-01 15:49:53.145 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 5500 / 14042
2023-11-01 15:51:11.239 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 5600 / 14042
2023-11-01 15:53:45.266 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 5700 / 14042
2023-11-01 15:57:30.920 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 5800 / 14042
2023-11-01 16:00:36.314 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 5900 / 14042
2023-11-01 16:02:56.688 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 6000 / 14042
2023-11-01 16:05:06.448 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 6100 / 14042
2023-11-01 16:05:38.043 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 6200 / 14042
2023-11-01 16:06:09.715 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 6300 / 14042
2023-11-01 16:06:43.820 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 6400 / 14042
2023-11-01 16:07:34.317 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 6500 / 14042
2023-11-01 16:08:28.652 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 6600 / 14042
2023-11-01 16:09:09.135 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 6700 / 14042
2023-11-01 16:09:53.249 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 6800 / 14042
2023-11-01 16:10:48.776 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 6900 / 14042
2023-11-01 16:11:32.683 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 7000 / 14042
2023-11-01 16:12:06.292 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 7100 / 14042
2023-11-01 16:12:47.447 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 7200 / 14042
2023-11-01 16:13:27.456 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 7300 / 14042
2023-11-01 16:14:02.812 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 7400 / 14042
2023-11-01 16:14:33.121 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 7500 / 14042
2023-11-01 16:15:03.757 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 7600 / 14042
2023-11-01 16:15:34.387 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 7700 / 14042
2023-11-01 16:16:04.630 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 7800 / 14042
2023-11-01 16:16:35.483 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 7900 / 14042
2023-11-01 16:17:06.651 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 8000 / 14042
2023-11-01 16:17:37.300 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 8100 / 14042
2023-11-01 16:18:13.361 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 8200 / 14042
2023-11-01 16:19:01.642 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 8300 / 14042
2023-11-01 16:19:49.217 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 8400 / 14042
2023-11-01 16:20:36.990 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 8500 / 14042
2023-11-01 16:21:39.157 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 8600 / 14042
2023-11-01 16:22:43.431 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 8700 / 14042
2023-11-01 16:23:47.603 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 8800 / 14042
2023-11-01 16:24:51.852 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 8900 / 14042
2023-11-01 16:25:56.073 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 9000 / 14042
2023-11-01 16:27:00.416 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 9100 / 14042
2023-11-01 16:28:04.675 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 9200 / 14042
2023-11-01 16:29:08.957 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 9300 / 14042
2023-11-01 16:30:13.219 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 9400 / 14042
2023-11-01 16:31:13.376 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 9500 / 14042
2023-11-01 16:32:13.108 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 9600 / 14042
2023-11-01 16:33:13.201 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 9700 / 14042
2023-11-01 16:33:49.798 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 9800 / 14042
2023-11-01 16:34:22.598 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 9900 / 14042
2023-11-01 16:34:54.803 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 10000 / 14042
2023-11-01 16:35:41.703 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 10100 / 14042
2023-11-01 16:36:33.327 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 10200 / 14042
2023-11-01 16:37:25.144 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 10300 / 14042
2023-11-01 16:38:26.162 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 10400 / 14042
2023-11-01 16:39:35.512 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 10500 / 14042
2023-11-01 16:40:45.395 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 10600 / 14042
2023-11-01 16:43:01.612 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 10700 / 14042
2023-11-01 16:45:46.136 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 10800 / 14042
2023-11-01 16:48:31.033 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 10900 / 14042
2023-11-01 16:51:19.512 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 11000 / 14042
2023-11-01 16:54:05.041 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 11100 / 14042
2023-11-01 16:56:49.435 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 11200 / 14042
2023-11-01 16:59:37.874 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 11300 / 14042
2023-11-01 17:02:23.648 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 11400 / 14042
2023-11-01 17:05:10.405 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 11500 / 14042
2023-11-01 17:07:53.403 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 11600 / 14042
2023-11-01 17:10:40.916 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 11700 / 14042
2023-11-01 17:13:27.167 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 11800 / 14042
2023-11-01 17:16:12.283 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 11900 / 14042
2023-11-01 17:18:58.394 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 12000 / 14042
2023-11-01 17:21:44.806 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 12100 / 14042
2023-11-01 17:24:13.629 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 12200 / 14042
2023-11-01 17:26:05.866 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 12300 / 14042
2023-11-01 17:27:59.176 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 12400 / 14042
2023-11-01 17:29:16.032 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 12500 / 14042
2023-11-01 17:30:09.295 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 12600 / 14042
2023-11-01 17:31:02.808 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 12700 / 14042
2023-11-01 17:31:57.289 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 12800 / 14042
2023-11-01 17:32:51.952 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 12900 / 14042
2023-11-01 17:33:46.310 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 13000 / 14042
2023-11-01 17:34:33.883 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 13100 / 14042
2023-11-01 17:35:46.639 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 13200 / 14042
2023-11-01 17:37:44.793 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 13300 / 14042
2023-11-01 17:39:43.249 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 13400 / 14042
2023-11-01 17:40:30.682 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 13500 / 14042
2023-11-01 17:41:14.968 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 13600 / 14042
2023-11-01 17:41:56.764 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 13700 / 14042
2023-11-01 17:42:31.685 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 13800 / 14042
2023-11-01 17:43:04.739 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 13900 / 14042
2023-11-01 17:43:31.278 | DEBUG    | evaluate.evaluator:evaluate_MMLU:24 - Step: 14000 / 14042
2023-11-01 17:43:42.452 | INFO     | evaluate.evaluator:evaluate_MMLU:37 - MMLU5-shots Acc: 0.4426719840478564
System tokens per second:  8192
avg_iter_time(s): 1.6564604701755803
Tokens/p/s:  4945.4847534826295
MFU:  0.444
Key: avg_train_prep, Value: 5.4374237060546875
Key: avg_train_loss, Value: 1.692576289176941
Key: avg_epoch_time, Value: 5152.036598856251
Key: avg_checkpoint_time, Value: 0
# converting the fp32 parameters to fp16
fp16 = True # only support true now

# using the nvidia apex
ddp_type = "apex"
train_batch_size = 8
eval_batch_size = 8

# Optimized primitives for inter-GPU communication
dist_backend = "nccl"

lr = 1e-5
weight_decay = 0.1
adam_beta1 = 0.9
adam_beta2 = 0.999
adam_eps = 1e-08

# Number of updates steps to accumualte before performing a backward/update pass.
gradient_accumulation_steps = 1
warmup = 0.1
lr_decay_ratio = 0.1

# number of iterations to decay LR over, If None defaults to `--train-iters`*`--epochs`
lr_decay_iters = 4338

# frequency of logging loss. If not positive, no logging is provided for training loss
log_freq = 1

training_event = None


# Total number of training samples to run.
max_samples_termination = 1388270 * 4
target_accuracy = 0.8

# For example, on A100-40G GPU, if we use 2 nodes, 16 GPUs, set
# *_batch_size = 8, max_samples_termination = 5553080,
# the model will be converged in about 750 steps in about 24 minutes.

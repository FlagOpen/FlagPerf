from training_event import ApexTrainingEvent
from config_common import *

# converting the fp32 parameters to fp16
fp16 = True # only support True now

# Optimized primitives for inter-GPU communication
dist_backend = "nccl"

# early stopping point
target_embedding_average = 0.91 

# Number of updates steps to accumualte before performing a backward/update pass.
gradient_accumulation_steps = 1 

train_batch_size = 32
eval_batch_size = train_batch_size
max_steps = 4000

warmup = 0.2
learning_rate = 0.0005

# LAMB beta1.
beta_1: float = 0.9

# LAMB beta2.
beta_2: float = 0.99 

# term added to the denominator to improve numerical stability
eps: float = 1e-08 

seed = 23333

training_event = None

# For example, on A100-40G GPU, if we use 2 nodes, 16 GPUs, set
# *_batch_size = 32, max_steps = 10000, max_samples_termination = 43912600,
# the model will be converged in about 2.76k steps in about 24 minutes.

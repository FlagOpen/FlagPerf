![FlagAI](assets/imgs/logo.png)
----------
### FlagPerf
[![Lint Code Base](https://github.com/FlagOpen/FlagPerf/actions/workflows/super-linter.yml/badge.svg)](https://github.com/FlagOpen/FlagPerf/actions/workflows/super-linter.yml)

FlagPerf æ˜¯æ™ºæºç ”ç©¶é™¢è”åˆAIç¡¬ä»¶å‚å•†å…±å»ºçš„ä¸€ä½“åŒ–AIç¡¬ä»¶è¯„æµ‹å¼•æ“ï¼Œæ—¨åœ¨å»ºç«‹ä»¥äº§ä¸šå®è·µä¸ºå¯¼å‘çš„æŒ‡æ ‡ä½“ç³»ï¼Œè¯„æµ‹AIç¡¬ä»¶åœ¨è½¯ä»¶æ ˆç»„åˆï¼ˆæ¨¡å‹+æ¡†æ¶+ç¼–è¯‘å™¨ï¼‰ä¸‹çš„å®é™…èƒ½åŠ›ã€‚æˆ‘ä»¬å¸Œæœ›æ¢ç´¢å¼€æºã€å¼€æ”¾ã€çµæ´»ã€å…¬æ­£ã€å®¢è§‚çš„AIèŠ¯ç‰‡è¯„æµ‹ä½“ç³»ï¼Œæä¾›è¡Œä¸šä»·å€¼ï¼Œä¿ƒè¿›AIäº§ä¸šç”Ÿæ€å‘å±•ã€‚

----------

### è¯„æµ‹æ–¹æ¡ˆåŠç‰¹æ€§

![cooperation](assets/imgs/frame.png)

1. æ„å»ºå¤šç»´åº¦è¯„æµ‹æŒ‡æ ‡ä½“ç³»ï¼Œä¸æ­¢å…³æ³¨â€œè€—æ—¶â€:
  FlagPerf æŒ‡æ ‡ä½“ç³»é™¤äº†è¡¡é‡â€œèŠ¯ç‰‡èƒ½å¦æ”¯æŒç‰¹å®šæ¨¡å‹è®­ç»ƒâ€çš„åŠŸèƒ½æ­£ç¡®æ€§æŒ‡æ ‡ä¹‹å¤–ï¼Œè¿˜åŒ…å«æ›´å¤šç»´åº¦çš„æ€§èƒ½æŒ‡æ ‡ã€èµ„æºä½¿ç”¨æŒ‡æ ‡ä»¥åŠç”Ÿæ€é€‚é…èƒ½åŠ›æŒ‡æ ‡ç­‰ã€‚
æŒ‡æ ‡è¯¦ç»†ä»‹ç»è§
[FlagPerf v1.0 å‘å¸ƒï¼Œæ„å»ºå¼€æºå¼€æ”¾çš„AIç¡¬ä»¶è¯„æµ‹ç”Ÿæ€](https://mp.weixin.qq.com/s/rwTFsthioBty5W2P-Lg9iw)

2. æ”¯æŒå¤šæ ·ä¾‹åœºæ™¯åŠä»»åŠ¡ï¼Œè¦†ç›–å¤§æ¨¡å‹è®­ç»ƒæ¨ç†åœºæ™¯
FlagPerf å·²ç»æ¶µç›–è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­éŸ³ç­‰é¢†åŸŸçš„20ä½™ä¸ªç»å…¸æ¨¡å‹ï¼Œ80ä½™ä¸ªè®­ç»ƒæ ·ä¾‹ï¼Œæ”¯æŒè¯„æµ‹AIç¡¬ä»¶çš„è®­ç»ƒå’Œæ¨ç†èƒ½åŠ›ï¼Œä»¥åŠå¤§æ¨¡å‹åœºæ™¯çš„æ¨ç†ä»»åŠ¡è¯„æµ‹ã€‚

3. æ”¯æŒå¤šè®­ç»ƒæ¡†æ¶åŠæ¨ç†å¼•æ“ï¼Œçµæ´»è¿æ¥AIç¡¬ä»¶ä¸è½¯ä»¶ç”Ÿæ€

    åœ¨è®­ç»ƒä»»åŠ¡åœºæ™¯ä¸­ï¼Œé™¤äº†æ”¯æŒ PyTorchã€TensorFlowï¼ŒFlagPerf è¿˜åœ¨ç§¯æä¸ PaddlePaddleã€MindSpore ç ”å‘å›¢é˜Ÿå¯†åˆ‡é…åˆã€‚ä½œä¸ºå›½äº§è®­ç»ƒæ¡†æ¶çš„é¢†å†›è€…ï¼Œç™¾åº¦ Paddleå›¢é˜Ÿã€åä¸ºæ˜‡æ€MindSpore å›¢é˜Ÿæ­£åœ¨å°† Llamaã€GPT3 ç­‰æ˜æ˜Ÿæ¨¡å‹é›†æˆè‡³ FlagPerf æµ‹è¯•æ ·ä¾‹é›†ã€‚

    åœ¨æ¨ç†ä»»åŠ¡åœºæ™¯ä¸­ï¼ŒFlagPerf é€‚é…äº†å¤šå®¶èŠ¯ç‰‡å‚å•†å’Œè®­ç»ƒæ¡†æ¶ç ”å‘å›¢é˜Ÿçš„æ¨ç†åŠ é€Ÿå¼•æ“ï¼Œä»¥æ›´çµæ´»åœ°è¿æ¥AIç¡¬ä»¶ä¸è½¯ä»¶ç”Ÿæ€ï¼Œæ‹“å®½è¯„æµ‹çš„è¾¹ç•Œå’Œæ•ˆç‡ï¼Œå¦‚è‹±ä¼Ÿè¾¾TensorRTã€æ˜†ä»‘èŠ¯XTCLï¼ˆXPU Tensor Compilation Libraryï¼‰ã€å¤©æ•°æ™ºèŠ¯IxRTï¼ˆIluvatar CoreX RunTimeï¼‰ã€PyTorch TorchInductorã€‚

4. æ”¯æŒå¤šæµ‹è¯•ç¯å¢ƒï¼Œç»¼åˆè€ƒå¯Ÿå•å¡ã€å•æœºã€å¤šæœºæ€§èƒ½

    ä¸ºå…¨é¢è¯„ä¼°å›½äº§AIèŠ¯ç‰‡å¤šæ ·æ€§ã€å¯æ‰©å±•æ€§ã€å®é™…åº”ç”¨æ¨¡æ‹Ÿæƒ…å†µï¼ŒFlagPerf è®¾å®šäº†å•å¡ã€å•æœºï¼ˆé€šå¸¸æ˜¯8å¡ï¼‰ã€å¤šæœºä¸‰ä¸ªæµ‹è¯•ç¯å¢ƒï¼Œä¸ºä¸åŒçš„æµ‹è¯•ç¯å¢ƒåŒ¹é…äº†ä¸åŒæµ‹è¯•æ ·ä¾‹åœºæ™¯å’Œä»»åŠ¡ã€‚
    
    >æ³¨ï¼šå½“å‰FlagPerfåœ¨ä¿è¯æµ‹è¯•ç¯å¢ƒé™¤èŠ¯ç‰‡å¤–å…¶ä»–æ¡ä»¶ä¸€è‡´çš„æƒ…å†µä¸‹ï¼Œè¿›è¡ŒèŠ¯ç‰‡æœ¬èº«çš„ç¦»çº¿æ‰¹å¤„ç†è¯„æµ‹ï¼Œæš‚ä¸æ”¯æŒé›†ç¾¤å’Œå®¢æˆ·ç«¯çš„æ€§èƒ½è¯„ä¼°ã€‚

5. ä¸¥æ ¼å®¡æ ¸å‚è¯„ä»£ç ï¼Œå…³æ³¨â€œç»“æœå…¬å¹³â€ï¼Œæ›´å…³æ³¨â€œè¿‡ç¨‹å…¬æ­£â€
    
    æµ‹è¯•ç”±æ™ºæºç ”ç©¶é™¢ä¸ä¼—å¤šèŠ¯ç‰‡å‚å•†è”åˆå±•å¼€ã€‚æ€»ä½“åŸåˆ™æ˜¯ç¡®ä¿å®¢è§‚ã€å…¬å¹³åœ°è¯„ä¼°èŠ¯ç‰‡çš„é€šç”¨æ€§èƒ½ï¼Œé™åˆ¶å‚å•†å¼€å±•æœ‰é’ˆå¯¹æ€§çš„å®šåˆ¶ä¼˜åŒ–ã€‚åœ¨ç¡®å®šæµ‹è¯•æ¨¡å‹ä¹‹åï¼Œé¦–å…ˆç”±èŠ¯ç‰‡å‚å•†è¿›è¡Œæ¨¡å‹é€‚é…ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¸­åªå…è®¸å‚å•†è¿›è¡Œåˆ†é•œåƒç¯å¢ƒã€æ‰¹æ•°æ®é‡ï¼ˆbatch sizeï¼‰ç­‰å’Œç¡¬ä»¶æ‰§è¡Œå¼ºç›¸å…³çš„æ–¹é¢çš„ä»£ç ä¿®æ”¹ï¼Œä»¥ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿåœ¨èŠ¯ç‰‡ä¸Šé«˜æ•ˆè¿è¡Œã€‚å…¶æ¬¡ç”±æ™ºæºç ”ç©¶é™¢ä¾æ‰˜åŸºå‡†æµ‹è¯•å¹³å°FlagPerfå¯¹èŠ¯ç‰‡èƒ½åŠ›å¼€å±•æµ‹è¯•ï¼Œå¹¶ç¡®ä¿æµ‹è¯•è¿‡ç¨‹é¡ºåˆ©ï¼ŒèŠ¯ç‰‡æ€§èƒ½å’Œç¨³å®šæ€§å¾—åˆ°æœ€ä½³å‘æŒ¥ã€‚åŒæ—¶ï¼Œæ‰€æœ‰æµ‹è¯•ä»£ç å‡å·²å¼€æºï¼Œæµ‹è¯•è¿‡ç¨‹ã€æ•°æ®å¯å¤ç°ã€‚

    ğŸ¯ æœªæ¥æ™ºæºåŠä¼—å¤šAIç¡¬ä»¶ã€æ¡†æ¶å›¢é˜Ÿè¿˜å°†å…±åŒæ‹“å±•FlagPerfçš„è¯„æµ‹åœºæ™¯ï¼Œå¦‚å¼€å±•é›†ç¾¤æ€§èƒ½çš„æ•´ä½“è¯„ä¼°ï¼Œä»¥æ›´å…¨é¢çš„è¯„ä¼°å›½äº§è½¯ç¡¬ä»¶çš„æ€§èƒ½ã€‚

### åŠ¨æ€
  - [31 Oct 2023]æ”¯æŒTorch-Aquila 7Bé¢„è®­ç»ƒï¼Œ#299
  - [27 Oct 2023]æ”¯æŒTorch-llama2 7Bé¢„è®­ç»ƒï¼Œ#289
  - [7 Oct 2023]æ”¯æŒPaddle-GPT3 é¢„è®­ç»ƒï¼Œ#233
  - [27 Sep 2023]å‘å¸ƒv1.0ç‰ˆæœ¬ï¼Œæ”¯æŒ20ä½™ä¸ªç»å…¸æ¨¡å‹ï¼Œ50ä½™ä¸ªè®­ç»ƒæ ·ä¾‹ï¼Œæ”¯æŒå¤šå®¶èŠ¯ç‰‡å‚å•†çš„è®­ç»ƒæˆ–æ¨ç†è¯„æµ‹ #v1.0
  - [3 Aug 2023]æ”¯æŒæ¨ç†æ¡†æ¶, æ”¯æŒå¸¸è§åŸºç¡€æ¨¡å‹çš„ç¦»çº¿æ‰¹æ¨ç†è¯„æµ‹ #136
  - [8 Feb 2023]æ”¯æŒTensorflowæ¡†æ¶#7
  - [6 Feb 2023]æ˜†ä»‘èŠ¯ä½œä¸ºåˆä½œå‚å•†è¿›å…¥å…±å»ºç”Ÿæ€ #6
  - [Dec 2022]å¤©æ•°æ™ºèŠ¯ã€ç™¾åº¦PaddlePaddleä½œä¸ºæœ€æ—©ä¸€æ‰¹å‚å•†å‚ä¸åˆç‰ˆå…±å»ºå¼€å‘
### æ”¯æŒåˆ—è¡¨


### å¿«é€Ÿä¸Šæ‰‹

#### åŸºç¡€ç¯å¢ƒç¡®è®¤
1. å®‰è£…dockerï¼Œpython
2. ç¡®ä¿ç¡¬ä»¶é©±åŠ¨ã€ç½‘ç»œã€ç¡¬ä»¶è™šæ‹ŸåŒ–ç­‰æœåŠ¡å™¨åŸºç¡€é…ç½®é½å…¨
    1. ç¡®ä¿å¯è¿ä¸­å›½å¤§é™†å¯è®¿é—®ç½‘ç«™ï¼Œé€Ÿç‡æ­£å¸¸
    2. ç¡®ä¿å¯åœ¨å®¹å™¨å†…æ‰¾åˆ°ç¡¬ä»¶
    3. ç¡®ä¿å„æœåŠ¡å™¨é—´rootå¸å·çš„sshä¿¡ä»»å…³ç³»å’Œsudoå…å¯†

#### è®­ç»ƒå¯åŠ¨è¯´æ˜

1. ä¸‹è½½FlagPerfå¹¶éƒ¨ç½²

    ```bash
    # ç¡®ä¿å„æœåŠ¡å™¨é—´rootå¸å·çš„sshä¿¡ä»»å…³ç³»å’Œsudoå…å¯†é…ç½®
    git clone https://github.com/FlagOpen/FlagPerf.git
    cd FlagPerf/training/
    pip3 install -r requirements.txt
    ```
2. ä¿®æ”¹æœºå™¨é…ç½®æ–‡ä»¶
    ```bash
    cd Flagperf/training/
    vim run_benchmarks/config/cluster_conf.py
    ```
    é›†ç¾¤é…ç½®æ–‡ä»¶ä¸»è¦åŒ…æ‹¬é›†ç¾¤ä¸»æœºåˆ—è¡¨å’ŒSSHç«¯å£ï¼Œä¿®æ”¹HOSTSå’ŒSSH_PORTä¸ºæœºå™¨å®é™…åœ°å€

    ```bash
    '''Cluster configs'''
    #Hosts to run the benchmark. Each item is an IP address or a hostname.
    HOSTS = ["10.1.2.3", "10.1.2.4", "10.1.2.5", "10.1.2.6"]
    #ssh connection port
    SSH_PORT = "22"
    ```

3. ä¿®æ”¹æ¨¡å‹é…ç½®æ–‡ä»¶
    ```bash
    cd Flagperf/training/
    vim run_benchmarks/config/test_conf.py
    ```
    å¿…æ”¹é¡¹ï¼š

    ```bash
    VENDOR = "nvidia" #é€‰æ‹©æœ¬æ¬¡è¿è¡Œçš„ç¡¬ä»¶
    FLAGPERF_PATH="" # FlagPerfé¡¹ç›®è·¯å¾„ï¼Œå¦‚"/home/FlagPerf/training"
    CASES={} # æœ¬æ¬¡è¿è¡Œçš„æµ‹ä¾‹ï¼ŒæŒ‰ç…§å¯¹åº”æ¨¡å‹readmeå‡†å¤‡å¥½æ•°æ®ï¼Œä¿®æ”¹æ¨¡å‹å¯¹åº”çš„åœ°å€

    #å¦‚è¿è¡Œ"bert:pytorch_1.8:A100:1:8:1": "/raid/home_datasets_ckpt/bert/train/"ï¼Œéœ€è¦æŠŠ:åé¢çš„è·¯å¾„æ›¿æ¢ä¸ºæœ¬åœ°è·¯å¾„
    ```
4. å¯åŠ¨æµ‹è¯•

    ```bash
    python3 ./run_benchmarks/run.py
    sudo python3 ./run_benchmarks/run.py
    ```

5. æŸ¥çœ‹æ—¥å¿—

    ```bash
    cd result/run2023XXXX/è¿è¡Œæ¨¡å‹/

    # ls
    round1

    # ls round1/
    10.1.2.2_noderank0

    # cd 10.1.2.2_noderank0/

    # ls
    cpu_monitor.log     pwr_monitor.log  rank2.out.log  rank5.out.log  start_pytorch_task.log
    mem_monitor.log     rank0.out.log    rank3.out.log  rank6.out.log
    nvidia_monitor.log  rank1.out.log    rank4.out.log  rank7.out.log


    # tail -n 6 rank0.out.log
    [PerfLog] {"event": "STEP_END", "value": {"loss": 2.679504871368408, "embedding_average": 0.916015625, "epoch": 1, "end_training": true, "global_steps": 3397, "num_trained_samples": 869632, "learning_rate": 0.000175375, "seq/s": 822.455385237589}, "metadata": {"file": "/workspace/flagperf/training/benchmarks/cpm/pytorch/run_pretraining.py", "lineno": 127, "time_ms": 1669034171032, "rank": 0}}
    [PerfLog] {"event": "EVALUATE", "metadata": {"file": "/workspace/flagperf/training/benchmarks/cpm/pytorch/run_pretraining.py", "lineno": 127, "time_ms": 1669034171032, "rank": 0}}
    [PerfLog] {"event": "EPOCH_END", "metadata": {"file": "/workspace/flagperf/training/benchmarks/cpm/pytorch/run_pretraining.py", "lineno": 127, "time_ms": 1669034171159, "rank": 0}}
    [PerfLog] {"event": "TRAIN_END", "metadata": {"file": "/workspace/flagperf/training/benchmarks/cpm/pytorch/run_pretraining.py", "lineno": 136, "time_ms": 1669034171159, "rank": 0}}
    [PerfLog] {"event": "FINISHED", "value": {"e2e_time": 1661.6114165782928, "training_sequences_per_second": 579.0933420700227, "converged": true, "final_loss": 3.066718101501465, "final_mlm_accuracy": 0.920166015625, "raw_train_time": 1501.713, "init_time": 148.937}, "metadata": {"file": "/workspace/flagperf/training/benchmarks/cpm/pytorch/run_pretraining.py", "lineno": 158, "time_ms": 1669034171646, "rank": 0}}
    ```

#### æ¨ç†å¯åŠ¨è¯´æ˜
1. ä¸‹è½½FlagPerfå¹¶éƒ¨ç½²
    ```bash
    # å…ˆå„æœåŠ¡å™¨é—´rootå¸å·çš„sshä¿¡ä»»å…³ç³»å’Œsudoå…å¯†é…ç½®
    git clone https://github.com/FlagOpen/FlagPerf.git
    cd FlagPerf/inference/
    pip3 install -r requirements.txt
    ```

2. ä¿®æ”¹æœºå™¨é…ç½®æ–‡ä»¶

    ```bash
    cd Flagperf/inference/
    vim configs/host.yaml
    é›†ç¾¤é…ç½®æ–‡ä»¶ä¸»è¦åŒ…æ‹¬é›†ç¾¤ä¸»æœºåˆ—è¡¨å’ŒSSHç«¯å£ï¼Œä¿®æ”¹HOSTSå’ŒSSH_PORTä¸ºæœºå™¨å®é™…åœ°å€
    ```
    å¿…é¡»ä¿®æ”¹é¡¹
    ```bash
    FLAGPERF_PATH: "/home/FlagPerf/inference" #FlagPerf inference è·¯å¾„
    HOSTS: ["127.0.0.1"] # æœºå™¨åœ°å€
    VENDOR = "nvidia" #æµ‹è¯•æœºå™¨å¯¹è±¡ï¼Œnvidia/kunlunxin/iluvatar
    CASES:  #å¾…æµ‹caseï¼Œè®°å¾—ä¿®æ”¹æ•°æ®åœ°å€
        "resnet50:pytorch_1.13": "/raid/dataset/ImageNet/imagenet/val"
    ```

3. ç”¨æˆ·éœ€è¦æ ¹æ®è¯„æµ‹å¯¹è±¡ï¼Œé…ç½®configs/<case>/configuration.yamlï¼Œå¦‚ä¸ä¿®æ”¹å¯ç”¨é»˜è®¤é…ç½®
    ```bash
    batch_size: 256

    # 1 item(like 1 sequence, 1 image) flops
    # Attention! For transformer decoder like bert, 1 token cause 2*param flops, so we need 2*length*params like 2*512*0.33B here
    # format: a_1*a*2*...*a_nea_0,like 2*512*0.33e9(bert) or 4.12e9(resnet50)
    flops: 4.12e9
    fp16: true
    compiler: tensorrt
    num_workers: 8
    log_freq: 30
    repeat: 5
    # skip validation(will also skip create_model, export onnx). Assert exist_onnx_path != null
    no_validation: false
    # set a real onnx_path to use exist, or set it to anything but null to avoid export onnx manually(like torch-tensorrt)
    exist_onnx_path: null
    # set a exist path of engine file like resnet50.trt/resnet50.plan/resnet50.engine
    exist_compiler_path: null
    ```
    å¿…æ”¹é¡¹ï¼š
    ```bash
    VENDOR = "nvidia" #é€‰æ‹©æœ¬æ¬¡è¿è¡Œçš„ç¡¬ä»¶
    FLAGPERF_PATH="" # FlagPerfé¡¹ç›®è·¯å¾„ï¼Œå¦‚"/home/FlagPerf/training"
    CASES={} # æœ¬æ¬¡è¿è¡Œçš„æµ‹ä¾‹ï¼ŒæŒ‰ç…§å¯¹åº”æ¨¡å‹readmeå‡†å¤‡å¥½æ•°æ®ï¼Œä¿®æ”¹æ¨¡å‹å¯¹åº”çš„åœ°å€
    #å¦‚è¿è¡Œ"bert:pytorch_1.8:A100:1:8:1": "/raid/home_datasets_ckpt/bert/train/"ï¼Œéœ€è¦æŠŠ:åé¢çš„è·¯å¾„æ›¿æ¢ä¸ºæœ¬åœ°è·¯å¾„
    ```

4. å¯åŠ¨æµ‹è¯•
    ```bash
    sudo python inference/run.py
    ```
    
- æ›´å¤šè®­ç»ƒ/æ¨ç†è¯´æ˜è§[è®­ç»ƒæ–‡æ¡£](https://github.com/FlagOpen/FlagPerf/tree/main/training/README.md) [æ¨ç†æ–‡æ¡£](https://github.com/FlagOpen/FlagPerf/blob/main/docs/dev/inference-case-doc.md)


### å¦‚ä½•å‚ä¸å…±å»ºã€to å¼€å‘è€…ã€‘
  - å¼€å‘è€…æ–‡ç« ï¼šæ›´å¤šæ“ä½œæ•™ç¨‹è§ docs-zh
  - å‚ä¸å…±å»ºå¤§æ¦‚çš„å·¥ä½œé‡

    ä¸ºäº†æ›´ç›´è§‚çš„å±•ç¤ºå‚å•†å‚ä¸å…±å»ºçš„å®é™…å·¥ä½œé‡ï¼Œä¸‹é¢ç»™å‡º6ä¸ªå·²ç»åˆå¹¶è¿›FlagPerfï¼Œé¢å‘ä¸åŒç‰¹å¾å‚å•†çš„Pull Requestï¼š
    1. å½“æŸå‚å•†ç¬¬ä¸€æ¬¡å‚ä¸è®­ç»ƒé€‚é…ï¼Œéœ€è¦é€‚é…çš„å†…å®¹è¾ƒå¤šã€‚é™¤äº†é€‚é…caseå¤–ï¼Œè¿˜åŒ…æ‹¬å‚å•†çš„dockerfileã€monitorç­‰ï¼Œå¦‚https://github.com/FlagOpen/FlagPerf/pull/246
    2. å½“æŸå‚å•†åç»­å‚ä¸è®­ç»ƒé€‚é…æ—¶ï¼Œå¦‚å‚å•†ä»¥cudaå…¼å®¹è·¯çº¿è®¾è®¡è½¯ç¡¬ä»¶ï¼Œå…¸å‹é€‚é…caseå¦‚https://github.com/FlagOpen/FlagPerf/pull/170
    3. å½“æŸå‚å•†åç»­å‚ä¸è®­ç»ƒé€‚é…æ—¶ï¼Œå¦‚å‚å•†ä¸å…¼å®¹cudaï¼Œåˆ™éœ€è¦é¢å¤–ä¿®æ”¹åç«¯é€šä¿¡æ–¹æ¡ˆç­‰ç­‰ã€‚å…¸å‹é€‚é…caseå¦‚https://github.com/FlagOpen/FlagPerf/pull/288ã€‚å½“caseè¾ƒå¤æ‚æ—¶ï¼Œå¯èƒ½éœ€è¦é‡å†™éƒ¨åˆ†è®¡ç®—æ–¹å¼ã€åŠç²¾åº¦æ¥å£ç­‰ï¼Œå¦‚https://github.com/FlagOpen/FlagPerf/pull/158
    4. å½“æŸå‚å•†ç¬¬ä¸€æ¬¡å‚ä¸æ¨ç†é€‚é…ï¼Œéœ€è¦é€‚é…çš„å†…å®¹è¾ƒå¤šã€‚é™¤äº†é€‚é…caseå¤–ï¼Œè¿˜åŒ…æ‹¬å‚å•†çš„dockerfileã€ç¼–è¯‘å™¨å®ç°æ–¹å¼ã€monitorç­‰ï¼Œå¦‚https://github.com/FlagOpen/FlagPerf/pull/256
    5. å½“æŸå‚å•†åç»­å‚ä¸æ¨ç†é€‚é…æ—¶ï¼Œé€šå¸¸ä¸éœ€è¦é€‚é…å·¥ä½œé‡ã€ä»…éœ€è¿è¡Œè½¯ä»¶å®Œæˆæµ‹è¯•ã€‚å¦‚https://github.com/FlagOpen/FlagPerf/pull/227



### å½“å‰åˆä½œä¼™ä¼´

    æœ¬é¡¹ç›®ç›®å‰ç”±åŒ—äº¬æ™ºæºäººå·¥æ™ºèƒ½ç ”ç©¶é™¢ã€å¤©æ•°æ™ºèŠ¯ã€ç™¾åº¦PaddlePaddleã€æ˜†ä»‘èŠ¯ã€åä¸ºæ˜‡è…¾ã€åä¸ºæ˜‡æ€MindSporeã€æ‘©å°”çº¿ç¨‹ã€è…¾è®¯ä¹éœ„å…±åŒå»ºè®¾ä¸­ã€‚

    è¯šé‚€å„æ¡†æ¶ã€èŠ¯ç‰‡ã€ç¼–è¯‘å™¨å›¢é˜Ÿä¸ä¸ªäººå‚ä¸ï¼

![cooperation](assets/imgs/logos.png)

### è”ç³»æˆ‘ä»¬

flagperf@baai.ac.cn
### è®¸å¯è¯
æœ¬é¡¹ç›®åŸºäºApache 2.0 licenseã€‚ 
<br>æœ¬é¡¹ç›®çš„ä»£ç æ¥æºäºä¸åŒçš„ä»£ç ä»“åº“ï¼Œå…³äºå„æ¨¡å‹æµ‹è¯•Caseçš„æƒ…å†µï¼Œè¯·å‚è€ƒå„æ¨¡å‹æµ‹è¯•Caseç›®å½•çš„æ–‡æ¡£ã€‚
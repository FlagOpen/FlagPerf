![FlagAI](assets/imgs/logo.png)
----------
## FlagPerfæ˜¯ä»€ä¹ˆ
[![Lint Code Base](https://github.com/FlagOpen/FlagPerf/actions/workflows/super-linter.yml/badge.svg)](https://github.com/FlagOpen/FlagPerf/actions/workflows/super-linter.yml)

**FlagPerfæ˜¯æ™ºæºç ”ç©¶é™¢è”åˆAIç¡¬ä»¶å‚å•†å…±å»ºçš„ä¸€ä½“åŒ–AIç¡¬ä»¶è¯„æµ‹å¼•æ“ï¼Œæ—¨åœ¨å»ºç«‹ä»¥äº§ä¸šå®è·µä¸ºå¯¼å‘çš„æŒ‡æ ‡ä½“ç³»ï¼Œè¯„æµ‹AIç¡¬ä»¶åœ¨è½¯ä»¶æ ˆç»„åˆï¼ˆæ¨¡å‹+æ¡†æ¶+ç¼–è¯‘å™¨ï¼‰ä¸‹çš„å®é™…èƒ½åŠ›ã€‚**

## ğŸ“£ FlagPerfè¯„æµ‹äº®ç‚¹

![cooperation](assets/imgs/overview.png)

1. **æ„å»ºå¤šç»´åº¦è¯„æµ‹æŒ‡æ ‡ä½“ç³»ï¼Œä¸æ­¢å…³æ³¨â€œè€—æ—¶â€:**

   FlagPerf æŒ‡æ ‡ä½“ç³»é™¤äº†è¡¡é‡â€œèŠ¯ç‰‡èƒ½å¦æ”¯æŒç‰¹å®šæ¨¡å‹è®­ç»ƒâ€çš„åŠŸèƒ½æ­£ç¡®æ€§æŒ‡æ ‡ä¹‹å¤–ï¼Œè¿˜åŒ…å«æ›´å¤šç»´åº¦çš„æ€§èƒ½æŒ‡æ ‡ã€èµ„æºä½¿ç”¨æŒ‡æ ‡ä»¥åŠç”Ÿæ€é€‚é…èƒ½åŠ›æŒ‡æ ‡ç­‰ã€‚
   
   > æŒ‡æ ‡è¯¦ç»†ä»‹ç»è§ [è¿™ç¯‡æ–‡ç« ](https://mp.weixin.qq.com/s/rwTFsthioBty5W2P-Lg9iw)

2. **æ”¯æŒå¤šæ ·ä¾‹åœºæ™¯åŠä»»åŠ¡ï¼Œè¦†ç›–å¤§æ¨¡å‹è®­ç»ƒæ¨ç†åœºæ™¯**

   FlagPerf å·²ç»æ¶µç›–è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­éŸ³ã€å¤šæ¨¡æ€ç­‰é¢†åŸŸçš„**30ä½™ä¸ªç»å…¸æ¨¡å‹ï¼Œ80ä½™ä¸ªè®­ç»ƒæ ·ä¾‹ï¼Œ**æ”¯æŒè¯„æµ‹AIç¡¬ä»¶çš„è®­ç»ƒå’Œæ¨ç†èƒ½åŠ›ï¼Œä»¥åŠå¤§æ¨¡å‹åœºæ™¯çš„æ¨ç†ä»»åŠ¡è¯„æµ‹ã€‚
   
3. **æ”¯æŒå¤šè®­ç»ƒæ¡†æ¶åŠæ¨ç†å¼•æ“ï¼Œçµæ´»è¿æ¥AIç¡¬ä»¶ä¸è½¯ä»¶ç”Ÿæ€**

   **åœ¨è®­ç»ƒä»»åŠ¡åœºæ™¯ä¸­**ï¼Œé™¤äº†æ”¯æŒ PyTorchã€TensorFlowï¼ŒFlagPerf è¿˜åœ¨ç§¯æä¸ PaddlePaddleã€MindSpore ç ”å‘å›¢é˜Ÿå¯†åˆ‡é…åˆã€‚ä½œä¸ºå›½äº§è®­ç»ƒæ¡†æ¶çš„é¢†å†›è€…ï¼Œç™¾åº¦ Paddleå›¢é˜Ÿã€åä¸ºæ˜‡æ€MindSpore å›¢é˜Ÿæ­£åœ¨å°† Llamaã€GPT3 ç­‰æ˜æ˜Ÿæ¨¡å‹é›†æˆè‡³ FlagPerf æµ‹è¯•æ ·ä¾‹é›†ã€‚

   **åœ¨æ¨ç†ä»»åŠ¡åœºæ™¯ä¸­**ï¼ŒFlagPerf é€‚é…äº†å¤šå®¶èŠ¯ç‰‡å‚å•†å’Œè®­ç»ƒæ¡†æ¶ç ”å‘å›¢é˜Ÿçš„æ¨ç†åŠ é€Ÿå¼•æ“ï¼Œä»¥æ›´çµæ´»åœ°è¿æ¥AIç¡¬ä»¶ä¸è½¯ä»¶ç”Ÿæ€ï¼Œæ‹“å®½è¯„æµ‹çš„è¾¹ç•Œå’Œæ•ˆç‡ï¼Œå¦‚è‹±ä¼Ÿè¾¾TensorRTã€æ˜†ä»‘èŠ¯XTCLï¼ˆXPU Tensor Compilation Libraryï¼‰ã€å¤©æ•°æ™ºèŠ¯IxRTï¼ˆIluvatar CoreX RunTimeï¼‰ã€PyTorch TorchInductorã€‚

4. **æ”¯æŒå¤šæµ‹è¯•ç¯å¢ƒï¼Œç»¼åˆè€ƒå¯Ÿå•å¡ã€å•æœºã€å¤šæœºæ€§èƒ½**

   ä¸ºå…¨é¢è¯„ä¼°å›½äº§AIèŠ¯ç‰‡å¤šæ ·æ€§ã€å¯æ‰©å±•æ€§ã€å®é™…åº”ç”¨æ¨¡æ‹Ÿæƒ…å†µï¼ŒFlagPerf è®¾å®šäº†å•å¡ã€å•æœºï¼ˆé€šå¸¸æ˜¯8å¡ï¼‰ã€å¤šæœºä¸‰ä¸ªæµ‹è¯•ç¯å¢ƒï¼Œä¸ºä¸åŒçš„æµ‹è¯•ç¯å¢ƒåŒ¹é…äº†ä¸åŒæµ‹è¯•æ ·ä¾‹åœºæ™¯å’Œä»»åŠ¡ã€‚

   > æ³¨ï¼šå½“å‰FlagPerfåœ¨ä¿è¯æµ‹è¯•ç¯å¢ƒé™¤èŠ¯ç‰‡å¤–å…¶ä»–æ¡ä»¶ä¸€è‡´çš„æƒ…å†µä¸‹ï¼Œè¿›è¡ŒèŠ¯ç‰‡æœ¬èº«çš„ç¦»çº¿æ‰¹å¤„ç†è¯„æµ‹ï¼Œæš‚ä¸æ”¯æŒé›†ç¾¤å’Œå®¢æˆ·ç«¯çš„æ€§èƒ½è¯„ä¼°ã€‚

5. **ä¸¥æ ¼å®¡æ ¸å‚è¯„ä»£ç ï¼Œå…³æ³¨â€œç»“æœå…¬å¹³â€ï¼Œæ›´å…³æ³¨â€œè¿‡ç¨‹å…¬æ­£â€**

   æµ‹è¯•ç”±æ™ºæºç ”ç©¶é™¢ä¸ä¼—å¤šèŠ¯ç‰‡å‚å•†è”åˆå±•å¼€ã€‚æ€»ä½“åŸåˆ™æ˜¯ç¡®ä¿å®¢è§‚ã€å…¬å¹³åœ°è¯„ä¼°èŠ¯ç‰‡çš„é€šç”¨æ€§èƒ½ï¼Œé™åˆ¶å‚å•†å¼€å±•æœ‰é’ˆå¯¹æ€§çš„å®šåˆ¶ä¼˜åŒ–ã€‚åœ¨ç¡®å®šæµ‹è¯•æ¨¡å‹ä¹‹åï¼Œé¦–å…ˆç”±èŠ¯ç‰‡å‚å•†è¿›è¡Œæ¨¡å‹é€‚é…ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¸­**åªå…è®¸å‚å•†è¿›è¡Œåˆ†å¸ƒå¼é€šä¿¡ã€æ‰¹æ•°æ®é‡ï¼ˆbatch sizeï¼‰ç­‰å’Œç¡¬ä»¶æ‰§è¡Œå¼ºç›¸å…³çš„æ–¹é¢çš„ä»£ç ä¿®æ”¹**ï¼Œä»¥ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿåœ¨èŠ¯ç‰‡ä¸Šé«˜æ•ˆè¿è¡Œã€‚å…¶æ¬¡ç”±æ™ºæºç ”ç©¶é™¢ä¾æ‰˜åŸºå‡†æµ‹è¯•å¹³å°FlagPerfå¯¹èŠ¯ç‰‡èƒ½åŠ›å¼€å±•æµ‹è¯•ï¼Œå¹¶ç¡®ä¿æµ‹è¯•è¿‡ç¨‹é¡ºåˆ©ï¼ŒèŠ¯ç‰‡æ€§èƒ½å’Œç¨³å®šæ€§å¾—åˆ°æœ€ä½³å‘æŒ¥ã€‚åŒæ—¶ï¼Œ**æ‰€æœ‰æµ‹è¯•ä»£ç å‡å·²å¼€æºï¼Œæµ‹è¯•è¿‡ç¨‹ã€æ•°æ®å¯å¤ç°ã€‚**

ğŸ¯ æœªæ¥æ™ºæºåŠä¼—å¤šAIç¡¬ä»¶ã€æ¡†æ¶å›¢é˜Ÿè¿˜å°†å…±åŒæ‹“å±•FlagPerfçš„è¯„æµ‹åœºæ™¯ï¼Œå¦‚å¼€å±•é›†ç¾¤æ€§èƒ½çš„æ•´ä½“è¯„ä¼°ï¼Œä»¥æ›´å…¨é¢çš„è¯„ä¼°å›½äº§è½¯ç¡¬ä»¶çš„æ€§èƒ½ã€‚

## News 

- [31 Oct 2023]æ”¯æŒTorch-Aquila 7Bé¢„è®­ç»ƒï¼Œ[#299](https://github.com/FlagOpen/FlagPerf/pull/136)
- [27 Oct 2023]æ”¯æŒTorch-llama2 7Bé¢„è®­ç»ƒï¼Œ[#289](https://github.com/FlagOpen/FlagPerf/pull/136)
- [7 Oct 2023]æ”¯æŒPaddle-GPT3 é¢„è®­ç»ƒï¼Œ[#233](https://github.com/FlagOpen/FlagPerf/pull/136)
- [27 Sep 2023]å‘å¸ƒv1.0ç‰ˆæœ¬ï¼Œæ”¯æŒ20ä½™ä¸ªç»å…¸æ¨¡å‹ï¼Œ50ä½™ä¸ªè®­ç»ƒæ ·ä¾‹ï¼Œæ”¯æŒå¤šå®¶èŠ¯ç‰‡å‚å•†çš„è®­ç»ƒæˆ–æ¨ç†è¯„æµ‹ [#v1.0](https://github.com/FlagOpen/FlagPerf/releases/tag/1.0)
- [3 Aug 2023]æ”¯æŒæ¨ç†æ¡†æ¶, æ”¯æŒå¸¸è§åŸºç¡€æ¨¡å‹çš„ç¦»çº¿æ‰¹æ¨ç†è¯„æµ‹ [#136](https://github.com/FlagOpen/FlagPerf/pull/136)
- [8 Feb 2023]æ”¯æŒTensorflowæ¡†æ¶[#7](https://github.com/FlagOpen/FlagPerf/pull/7)
- [6 Feb 2023]æ˜†ä»‘èŠ¯ä½œä¸ºåˆä½œå‚å•†è¿›å…¥å…±å»ºç”Ÿæ€ [#6](https://github.com/FlagOpen/FlagPerf/pull/6)
- [Dec 2022]å¤©æ•°æ™ºèŠ¯ã€ç™¾åº¦PaddlePaddleä½œä¸ºæœ€æ—©ä¸€æ‰¹å‚å•†å‚ä¸åˆç‰ˆå…±å»ºå¼€å‘

## æ”¯æŒåˆ—è¡¨

è®­ç»ƒåˆ—è¡¨ï¼š

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>ç¼–å·</th>
        <th>æ¨¡å‹åç§°</th>
      <th>æ¨¡å‹ç±»å‹</th>
      <th>è‹±ä¼Ÿè¾¾</th>
      <th>æ˜†ä»‘èŠ¯</th>
      <th>å¤©æ•°æ™ºèŠ¯</th>
      <th>æ‘©å°”çº¿ç¨‹</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
        <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/benchmarks/aquila2_7b">aquila2_7b</a></td>
      <td>NLP</td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/nvidia/aquila2_7b-flagscale">flagscaleï¼ˆmegatronï¼‰</a></td>
      <td>N/A</td>
      <td>N/A</td>
        <td>N/A</td>
    </tr>
    <tr>
      <td>2</td>
        <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/benchmarks/bert">bert</a></td>
      <td>NLP</td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/nvidia/bert-paddle">paddle</a>, <a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/nvidia/bert-pytorch">pytorch</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/kunlunxin/bert-paddle">paddle</a>, <a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/kunlunxin/bert-pytorch">pytorch</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/iluvatar/bert-paddle">paddle</a>, <a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/iluvatar/bert-pytorch">pytorch</a></td>
        <td>N/A</td>
    </tr>
    <tr>
      <td>3</td>
        <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/benchmarks/bert_hf">bert_hf</a></td>
      <td>NLP</td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/nvidia/bert_hf-pytorch">pytorch</a></td>
      <td>N/A</td>
      <td>N/A</td>
        <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/mthreads/bert_hf-pytorch">pytorch</a></td>
    </tr>
    <tr>
      <td>4</td>
        <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/benchmarks/bigtransfer">bigtransfer</a></td>
      <td>CV</td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/nvidia/bigtransfer-pytorch">pytorch</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/kunlunxin/bigtransfer-pytorch">pytorch</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/iluvatar/bigtransfer-pytorch">pytorch</a></td>
        <td>N/A</td>
    </tr>
    <tr>
      <td>5</td>
        <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/benchmarks/cpm">cpm</a></td>
      <td>LLM</td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/nvidia/cpm-pytorch">pytorch</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/kunlunxin/cpm-pytorch">pytorch</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/iluvatar/cpm-pytorch">pytorch</a></td>
        <td>N/A</td>
    </tr>
    <tr>
      <td>6</td>
        <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/benchmarks/detr">detr</a></td>
      <td>CV</td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/nvidia/detr-pytorch">pytorch</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/kunlunxin/detr-pytorch">pytorch</a></td>
      <td>N/A</td>
        <td>N/A</td>
    </tr>
    <tr>
      <td>7</td>
        <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/benchmarks/distilbert">distilbert</a></td>
      <td>NLP</td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/nvidia/distilbert-pytorch">pytorch</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/kunlunxin/distilbert-pytorch">pytorch</a></td>
      <td>N/A</td>
        <td>N/A</td>
    </tr>
    <tr>
     <td>8</td>
        <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/benchmarks/efficientnet">efficientnet</a></td>
      <td>CV</td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/nvidia/efficientnet-pytorch">pytorch</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/kunlunxin/efficientnet-pytorch">pytorch</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/iluvatar/efficientnet-pytorch">pytorch</a></td>
        <td>N/A</td>
    </tr>
    <tr>
      <td>9</td>
        <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/benchmarks/faster_rcnn">faster_rcnn</a></td>
      <td>CV</td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/nvidia/faster_rcnn-pytorch">pytorch</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/kunlunxin/faster_rcnn-pytorch">pytorch</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/iluvatar/faster_rcnn-pytorch">pytorch</a></td>
        <td>N/A</td>
    </tr>
    <tr>
      <td>10</td>
        <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/benchmarks/glm">glm</a></td>
      <td>LLM</td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/nvidia/glm-pytorch">pytorch</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/kunlunxin/glm-pytorch">pytorch</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/iluvatar/glm-pytorch">pytorch</a></td>
        <td>N/A</td>
    </tr>
    <tr>
      <td>11</td>
        <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/benchmarks/gpt2">gpt2</a></td>
      <td>LLM</td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/nvidia/gpt2-pytorch">pytorch</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/kunlunxin/gpt2-pytorch">pytorch</a></td>
      <td>N/A</td>
        <td>N/A</td>
    </tr>
    <tr>
     <td>12</td> 
        <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/benchmarks/gpt3_13B">gpt3_13B</a></td>
      <td>LLM</td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/nvidia/gpt3_13B-paddle">paddle</a></td>
      <td>N/A</td>
      <td>N/A</td>
        <td>N/A</td>
    </tr>
    <tr>
      <td>13</td>
        <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/benchmarks/gpt3_6.7B">gpt3_6.7B</a></td>
      <td>LLM</td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/nvidia/gpt3_6.7B-paddle">paddle</a></td>
      <td>N/A</td>
      <td>N/A</td>
        <td>N/A</td>
    </tr>
    <tr>
      <td>14</td>
        <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/benchmarks/llama1_13B">llama1_13B</a></td>
      <td>LLM</td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/nvidia/llama1_13B-paddle">paddle</a></td>
      <td>N/A</td>
      <td>N/A</td>
        <td>N/A</td>
    </tr>
    <tr>
      <td>15</td>
        <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/benchmarks/llama1_7B">llama1_7B</a></td>
      <td>LLM</td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/nvidia/llama1_7B-paddle">paddle</a></td>
      <td>N/A</td>
      <td>N/A</td>
        <td>N/A</td>
    </tr>
    <tr>
      <td>16</td>
        <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/benchmarks/llama2_7b">llama2_7b</a></td>
      <td>LLM</td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/nvidia/llama2_7b-deepspeed">deepspeed</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/pull/348">deepspeed</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/pull/343">deepspeed</a></td>
        <td><a href="https://github.com/FlagOpen/FlagPerf/pull/354">deepspeed</a></td>
    </tr>
      <tr>
      <td>17</td>
          <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/benchmarks/llama2_7b_finetune">llama2_7b_finetune</a></td>
      <td>LLM</td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/nvidia/llama2_7b_finetune-pytorch">pytorch</a></td>
      <td>N/A</td>
      <td>N/A</td>
      <td>N/A</td>
    </tr>
    <tr>
      <td>18</td>
        <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/benchmarks/longformer">longformer</a></td>
      <td>NLP</td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/nvidia/longformer-pytorch">pytorch</a></td>
        <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/kunlunxin/longformer-pytorch">pytorch</a></td>
      <td>N/A</td>
      <td>N/A</td>
    </tr>
    <tr>
      <td>19</td>
        <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/benchmarks/mask_rcnn">mask_rcnn</a></td>
      <td>CV</td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/nvidia/mask_rcnn-pytorch">pytorch</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/kunlunxin/mask_rcnn-pytorch">pytorch</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/iluvatar/mask_rcnn-pytorch">pytorch</a></td>
        <td>N/A</td>
    </tr>
    <tr>
      <td>20</td>
        <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/benchmarks/mobilenetv2">mobilenetv2</a></td>
      <td>CV</td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/nvidia/mobilenetv2-pytorch">pytorch</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/kunlunxin/mobilenetv2-pytorch">pytorch</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/iluvatar/mobilenetv2-pytorch">pytorch</a></td>
        <td>N/A</td>
    </tr>
    <tr>
      <td>21</td>
        <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/benchmarks/resnet50">resnet50</a></td>
      <td>CV</td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/nvidia/resnet50-pytorch">pytorch</a>, <a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/nvidia/resnet50-tensorflow2">tensorflow2</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/kunlunxin/resnet50-pytorch">pytorch</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/iluvatar/resnet50-pytorch">pytorch</a></td>
        <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/mthreads/resnet50-pytorch">pytorch</a></td>
    </tr>
    <tr>
      <td>22</td>
        <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/benchmarks/retinanet">retinanet</a></td>
      <td>CV</td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/nvidia/retinanet-pytorch">pytorch</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/kunlunxin/retinanet-pytorch">pytorch</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/iluvatar/retinanet-pytorch">pytorch</a></td>
        <td>N/A</td>
    </tr>
    <tr>
      <td>23</td>
        <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/benchmarks/swin_transformer">swin_transformer</a></td>
      <td>CV</td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/nvidia/swin_transformer-pytorch">pytorch</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/kunlunxin/swin_transformer-pytorch">pytorch</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/iluvatar/swin_transformer-pytorch">pytorch</a></td>
        <td>N/A</td>
    </tr>
    <tr>
      <td>24</td>
        <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/benchmarks/t5_small">t5_small</a></td>
      <td>NLP</td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/nvidia/t5_small-pytorch">pytorch</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/kunlunxin/t5_small-pytorch">pytorch</a></td>
      <td>N/A</td>
        <td>N/A</td>
    </tr>
    <tr>
      <td>25</td>
        <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/benchmarks/tacotron2">tacotron2</a></td>
      <td>Audio</td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/nvidia/tacotron2-pytorch">pytorch</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/kunlunxin/tacotron2-pytorch">pytorch</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/iluvatar/tacotron2-pytorch">pytorch</a></td>
        <td>N/A</td>
    </tr>
    <tr>
      <td>26</td>
        <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/benchmarks/transformer">transformer</a></td>
      <td>NLP</td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/nvidia/transformer-pytorch">pytorch</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/kunlunxin/transformer-pytorch">pytorch</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/iluvatar/transformer-pytorch">pytorch</a></td>
        <td>N/A</td>
    </tr>
    <tr>
      <td>27</td>
        <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/benchmarks/transformer_xl">transformer_xl</a></td>
      <td>NLP</td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/nvidia/transformer_xl-pytorch">pytorch</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/kunlunxin/transformer_xl-pytorch">pytorch</a></td>
      <td>N/A</td>
        <td>N/A</td>
    </tr>
    <tr>
      <td>28</td>
        <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/benchmarks/vit">vit</a></td>
      <td>CV</td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/nvidia/vit-pytorch">pytorch</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/kunlunxin/vit-pytorch">pytorch</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/iluvatar/vit-pytorch">pytorch</a></td>
        <td>N/A</td>
    </tr>
    <tr>
      <td>29</td>
        <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/benchmarks/wav2vec2">wav2vec2</a></td>
      <td>Audio</td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/nvidia/wav2vec2-pytorch">pytorch</a></td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/kunlunxin/wav2vec2-pytorch">pytorch</a></td>
      <td>N/A</td>
        <td>N/A</td>
    </tr>
    <tr>
      <td>30</td>
        <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/benchmarks/WaveGlow">WaveGlow</a></td>
      <td>Audio</td>
      <td><a href="https://github.com/FlagOpen/FlagPerf/tree/main/training/nvidia/WaveGlow-pytorch">pytorch</a></td>
      <td>N/A</td>
      <td>N/A</td>
        <td>N/A</td>
    </tr>
  </tbody>
</table>


æ¨ç†åˆ—è¡¨ï¼š

<table width="960" border="0" cellpadding="0" cellspacing="0" style='width:960pt;border-collapse:collapse;table-layout:fixed;'>
   <col width="73.60" style='mso-width-source:userset;mso-width-alt:3588;'/>
   <col width="70" style='mso-width-source:userset;mso-width-alt:3413;'/>
   <col width="200.75" style='mso-width-source:userset;mso-width-alt:9788;'/>
   <col width="195.80" style='mso-width-source:userset;mso-width-alt:9547;'/>
   <col width="185.40" style='mso-width-source:userset;mso-width-alt:9040;'/>
   <tr height="16.80" class="xl65" style='height:16.80pt;'>
   </tr>
   <tr height="16.80" style='height:16.80pt;'>
    <td class="xl65" x:str>ç¼–å·</td>
       <td class="xl65" x:str>æ¨¡å‹åç§°</td>
       <td class="xl65" x:str>æ¨¡å‹ç±»å‹</td>
    <td class="xl65" x:str>è‹±ä¼Ÿè¾¾</td>
    <td class="xl65" x:str>æ˜†ä»‘èŠ¯</td>
    <td class="xl65" x:str>å¤©æ•°æ™ºèŠ¯</td>
    <td class="xl65" x:str>è…¾è®¯ä¹éœ„</td>
   </tr>
   <tr height="16.80" style='height:16.80pt;'>
    <td class="xl65" x:str>1</td>
       <td class="xl65" height="33.60"  style='height:33.60pt;border-right:none;border-bottom:none;' x:str><a href="https://github.com/FlagOpen/FlagPerf/tree/main/inference/benchmarks/resnet50" style="text-decoration:none" target="_parent">resnet50</a></td>
       <td class="xl69" x:str>CV</td>
    <td class="xl69" x:str>f32/f16</td>
    <td class="xl69" x:str>f32/f16</td>
    <td class="xl69" x:str>f16</td>
       <td class="xl69" x:str>f16</td>
   </tr>
    <tr height="16.80" style='height:16.80pt;'>
    <td class="xl65" x:str>2</td>
        <td class="xl65" height="33.60"  style='height:33.60pt;border-right:none;border-bottom:none;' x:str><a href="https://github.com/FlagOpen/FlagPerf/tree/main/inference/benchmarks/bertLarge" style="text-decoration:none" target="_parent">BertLarge</a></td>
        <td class="xl69" x:str>NLP</td>
    <td class="xl69" x:str>f32/f16</td>
    <td class="xl69" x:str>W32A16</td>
    <td class="xl69" x:str>Incoming</td>
        <td class="xl69" x:str>N/A</td>
   </tr>
    <tr height="16.80" style='height:16.80pt;'>
    <td class="xl65" x:str>3</td>
        <td class="xl65" height="33.60"  style='height:33.60pt;border-right:none;border-bottom:none;' x:str><a href="https://github.com/FlagOpen/FlagPerf/tree/main/inference/benchmarks/vit_l_16" style="text-decoration:none" target="_parent">VisionTransformer</a></td>
        <td class="xl69" x:str>CV</td>
    <td class="xl69" x:str>f32/f16</td>
    <td class="xl69" x:str>W32A16</td>
    <td class="xl69" x:str>N/A</td>
        <td class="xl69" x:str>N/A</td>
   </tr>
    <tr height="16.80" style='height:16.80pt;'>
    <td class="xl65" x:str>4</td>
        <td class="xl65" height="33.60" style='height:33.60pt;border-right:none;border-bottom:none;' x:str><a href="https://github.com/FlagOpen/FlagPerf/tree/main/inference/benchmarks/yolov5" style="text-decoration:none" target="_parent">Yolov5_large</a></td>
        <td class="xl69" x:str>CV</td>
    <td class="xl69" x:str>f32/f16</td>
    <td class="xl69" x:str>f32</td>
    <td class="xl69" x:str>f16</td>
        <td class="xl69" x:str>N/A</td>
   </tr>
   <tr height="16.80" style='height:16.80pt;'>
    <td class="xl65" x:str>5</td>
       <td class="xl65" height="33.60"  style='height:33.60pt;border-right:none;border-bottom:none;' x:str><a href="https://github.com/FlagOpen/FlagPerf/tree/main/inference/benchmarks/stable_diffusion_v1_4" style="text-decoration:none" target="_parent">Stable Diffusion v1.4</a></td>
       <td class="xl69" x:str>MultiModal</td>
    <td class="xl69" x:str>f32/f16</td>
    <td class="xl69" x:str>f32</td>
    <td class="xl69" x:str>N/A</td>
       <td class="xl69" x:str>N/A</td>
   </tr>
    <tr height="16.80" style='height:16.80pt;'>
    <td class="xl65" x:str>6</td>
        <td class="xl65" height="33.60"  style='height:33.60pt;border-right:none;border-bottom:none;' x:str><a href="https://github.com/FlagOpen/FlagPerf/tree/main/inference/benchmarks/swinTransformer" style="text-decoration:none" target="_parent">SwinTransformer</td>
        <td class="xl69" x:str>CV</td>
    <td class="xl69" x:str>f32/f16</td>
    <td class="xl69" x:str>W32A16</td>
    <td class="xl69" x:str>N/A</td>
        <td class="xl69" x:str>N/A</td>
   </tr>
    <tr height="16.80" style='height:16.80pt;'>
    <td class="xl65" x:str>7</td>
        <td class="xl65" height="33.60" style='height:33.60pt;border-right:none;border-bottom:none;' x:str><a href="https://github.com/FlagOpen/FlagPerf/tree/main/inference/benchmarks/llama2_7b_mmlu" style="text-decoration:none" target="_parent">Llama2-7B-mmlu</td>
        <td class="xl69" x:str>NLP</td>
    <td class="xl69" x:str>f32/f16</td>
    <td class="xl69" x:str>N/A</td>
    <td class="xl69" x:str>N/A</td>
        <td class="xl69" x:str>N/A</td>
   </tr>
    <tr height="16.80" style='height:16.80pt;'>
    <td class="xl65" x:str>8</td>
        <td class="xl65" height="33.60" style='height:33.60pt;border-right:none;border-bottom:none;' x:str><a href="https://github.com/FlagOpen/FlagPerf/tree/main/inference/benchmarks/aquila_7b_mmlu" style="text-decoration:none" target="_parent">Aquila-7B-mmlu</td>
        <td class="xl69" x:str>NLP</td>
    <td class="xl69" x:str>fp16</td>
    <td class="xl69" x:str>N/A</td>
    <td class="xl69" x:str>N/A</td>
        <td class="xl69" x:str>N/A</td>
    </tr>
<tr height="16.80" style='height:16.80pt;'>
   <td class="xl65" x:str>9</td>
    <td class="xl65" height="33.60" style='height:33.60pt;border-right:none;border-bottom:none;' x:str><a href="https://github.com/FlagOpen/FlagPerf/tree/main/inference/benchmarks/sam" style="text-decoration:none" target="_parent">SegmentAnything</td>
        <td class="xl69" x:str>MultiModal</td>
    <td class="xl69" x:str>fp16</td>
    <td class="xl69" x:str>W32A16</td>
    <td class="xl69" x:str>N/A</td>
        <td class="xl69" x:str>N/A</td>
    </tr></table>


## å¦‚ä½•ä½¿ç”¨FlagPerfè¿›è¡ŒAIç¡¬ä»¶è¯„æµ‹

### åŸºç¡€ç¯å¢ƒç¡®è®¤

1. å®‰è£…dockerï¼Œpython
2. ç¡®ä¿ç¡¬ä»¶é©±åŠ¨ã€ç½‘ç»œã€ç¡¬ä»¶è™šæ‹ŸåŒ–ç­‰æœåŠ¡å™¨åŸºç¡€é…ç½®é½å…¨
   1. ç¡®ä¿å¯è¿ä¸­å›½å¤§é™†å¯è®¿é—®ç½‘ç«™ï¼Œé€Ÿç‡æ­£å¸¸
   2. ç¡®ä¿å¯åœ¨å®¹å™¨å†…æ‰¾åˆ°ç¡¬ä»¶
   3. ç¡®ä¿å„æœåŠ¡å™¨é—´rootå¸å·çš„sshä¿¡ä»»å…³ç³»å’Œsudoå…å¯†
   4. ç¡®ä¿monitorç›¸å…³å·¥å…·å·²å®‰è£…:åŒ…æ‹¬cpu(sysstat)ã€å†…å­˜(free)ã€åŠŸè€—(ipmitool)ã€ç³»ç»Ÿä¿¡æ¯(åŠ é€Ÿå¡çŠ¶æ€æŸ¥çœ‹å‘½ä»¤)ã€‚ä¾‹å¦‚ubuntuç³»ç»Ÿä¸­ï¼Œä½¿ç”¨apt install [sysstat/ipmitool]å®‰è£…

### è®­ç»ƒå¯åŠ¨è¯´æ˜

1. **ä¸‹è½½FlagPerfå¹¶éƒ¨ç½²**

```Bash
# å…ˆå„æœåŠ¡å™¨é—´rootå¸å·çš„sshä¿¡ä»»å…³ç³»å’Œsudoå…å¯†é…ç½®
git clone https://github.com/FlagOpen/FlagPerf.git
cd FlagPerf/training/
pip3 install -r requirements.txt
```

2. **ä¿®æ”¹æœºå™¨é…ç½®æ–‡ä»¶**

```Bash
cd Flagperf/training/
vim run_benchmarks/config/cluster_conf.py
```

é›†ç¾¤é…ç½®æ–‡ä»¶ä¸»è¦åŒ…æ‹¬é›†ç¾¤ä¸»æœºåˆ—è¡¨å’ŒSSHç«¯å£ï¼Œä¿®æ”¹`HOSTS`å’Œ`SSH_PORT`ä¸ºæœºå™¨å®é™…åœ°å€

```Bash
'''Cluster configs'''
#Hosts to run the benchmark. Each item is an IP address or a hostname.
HOSTS = ["10.1.2.3", "10.1.2.4", "10.1.2.5", "10.1.2.6"]
#ssh connection port
SSH_PORT = "22"
```

3. ä¿®æ”¹æ¨¡å‹é…ç½®æ–‡ä»¶

```Bash
cd Flagperf/training/
vim run_benchmarks/config/test_conf.py
```

å¿…æ”¹é¡¹ï¼š

```Bash
VENDOR = "nvidia" #é€‰æ‹©æœ¬æ¬¡è¿è¡Œçš„ç¡¬ä»¶
FLAGPERF_PATH="" # FlagPerfé¡¹ç›®è·¯å¾„ï¼Œå¦‚"/home/FlagPerf/training"
CASES={} # æœ¬æ¬¡è¿è¡Œçš„æµ‹ä¾‹ï¼ŒæŒ‰ç…§å¯¹åº”æ¨¡å‹readmeå‡†å¤‡å¥½æ•°æ®ï¼Œä¿®æ”¹æ¨¡å‹å¯¹åº”çš„åœ°å€
#å¦‚è¿è¡Œ"bert:pytorch_1.8:A100:1:8:1": "/raid/home_datasets_ckpt/bert/train/"ï¼Œéœ€è¦æŠŠ:åé¢çš„è·¯å¾„æ›¿æ¢ä¸ºæœ¬åœ°è·¯å¾„
```

4. å¯åŠ¨æµ‹è¯•

```Bash
python3 ./run_benchmarks/run.py
sudo python3 ./run_benchmarks/run.py
```

5. æŸ¥çœ‹æ—¥å¿—

```Bash
cd result/run2023XXXX/è¿è¡Œæ¨¡å‹/
# ls
round1
# ls round1/
10.1.2.2_noderank0
# cd 10.1.2.2_noderank0/
# ls
cpu_monitor.log     pwr_monitor.log  rank2.out.log  rank5.out.log  start_pytorch_task.log
mem_monitor.log     rank0.out.log    rank3.out.log  rank6.out.log
nvidia_monitor.log  rank1.out.log    rank4.out.log  rank7.out.log


# tail -n 6 rank0.out.log
[PerfLog] {"event": "STEP_END", "value": {"loss": 2.679504871368408, "embedding_average": 0.916015625, "epoch": 1, "end_training": true, "global_steps": 3397, "num_trained_samples": 869632, "learning_rate": 0.000175375, "seq/s": 822.455385237589}, "metadata": {"file": "/workspace/flagperf/training/benchmarks/cpm/pytorch/run_pretraining.py", "lineno": 127, "time_ms": 1669034171032, "rank": 0}}
[PerfLog] {"event": "EVALUATE", "metadata": {"file": "/workspace/flagperf/training/benchmarks/cpm/pytorch/run_pretraining.py", "lineno": 127, "time_ms": 1669034171032, "rank": 0}}
[PerfLog] {"event": "EPOCH_END", "metadata": {"file": "/workspace/flagperf/training/benchmarks/cpm/pytorch/run_pretraining.py", "lineno": 127, "time_ms": 1669034171159, "rank": 0}}
[PerfLog] {"event": "TRAIN_END", "metadata": {"file": "/workspace/flagperf/training/benchmarks/cpm/pytorch/run_pretraining.py", "lineno": 136, "time_ms": 1669034171159, "rank": 0}}
[PerfLog] {"event": "FINISHED", "value": {"e2e_time": 1661.6114165782928, "training_sequences_per_second": 579.0933420700227, "converged": true, "final_loss": 3.066718101501465, "final_mlm_accuracy": 0.920166015625, "raw_train_time": 1501.713, "init_time": 148.937}, "metadata": {"file": "/workspace/flagperf/training/benchmarks/cpm/pytorch/run_pretraining.py", "lineno": 158, "time_ms": 1669034171646, "rank": 0}}
```

### æ¨ç†å¯åŠ¨è¯´æ˜

1. **ä¸‹è½½FlagPerfå¹¶éƒ¨ç½²**

```Bash
# å…ˆå„æœåŠ¡å™¨é—´rootå¸å·çš„sshä¿¡ä»»å…³ç³»å’Œsudoå…å¯†é…ç½®
git clone https://github.com/FlagOpen/FlagPerf.git
cd FlagPerf/inference/
pip3 install -r requirements.txt
```

2. **ä¿®æ”¹æœºå™¨é…ç½®æ–‡ä»¶**

```Bash
cd Flagperf/inference/
vim configs/host.yaml
```

é›†ç¾¤é…ç½®æ–‡ä»¶ä¸»è¦åŒ…æ‹¬é›†ç¾¤ä¸»æœºåˆ—è¡¨å’ŒSSHç«¯å£ï¼Œä¿®æ”¹`HOSTS`å’Œ`SSH_PORT`ä¸ºæœºå™¨å®é™…åœ°å€

```Bash
#å¿…é¡»ä¿®æ”¹é¡¹
FLAGPERF_PATH: "/home/FlagPerf/inference" #FlagPerf inference è·¯å¾„
HOSTS: ["127.0.0.1"] # æœºå™¨åœ°å€
VENDOR = "nvidia" #æµ‹è¯•æœºå™¨å¯¹è±¡ï¼Œnvidia/kunlunxin/iluvatar
CASES:  #å¾…æµ‹caseï¼Œè®°å¾—ä¿®æ”¹æ•°æ®åœ°å€
    "resnet50:pytorch_1.13": "/raid/dataset/ImageNet/imagenet/val"
```

3. ç”¨æˆ·éœ€è¦æ ¹æ®è¯„æµ‹å¯¹è±¡ï¼Œé…ç½®configs/<case>/configuration.yamlï¼Œå¦‚ä¸ä¿®æ”¹å¯ç”¨é»˜è®¤é…ç½®

```Bash
batch_size: 256
# 1 item(like 1 sequence, 1 image) flops
# Attention! For transformer decoder like bert, 1 token cause 2*param flops, so we need 2*length*params like 2*512*0.33B here
# format: a_1*a*2*...*a_nea_0,like 2*512*0.33e9(bert) or 4.12e9(resnet50)
flops: 4.12e9
fp16: true
compiler: tensorrt
num_workers: 8
log_freq: 30
repeat: 5
# skip validation(will also skip create_model, export onnx). Assert exist_onnx_path != null
no_validation: false
# set a real onnx_path to use exist, or set it to anything but null to avoid export onnx manually(like torch-tensorrt)
exist_onnx_path: null
# set a exist path of engine file like resnet50.trt/resnet50.plan/resnet50.engine
exist_compiler_path: null
```

å¿…æ”¹é¡¹ï¼š

```Bash
VENDOR = "nvidia" #é€‰æ‹©æœ¬æ¬¡è¿è¡Œçš„ç¡¬ä»¶
FLAGPERF_PATH="" # FlagPerfé¡¹ç›®è·¯å¾„ï¼Œå¦‚"/home/FlagPerf/training"
CASES={} # æœ¬æ¬¡è¿è¡Œçš„æµ‹ä¾‹ï¼ŒæŒ‰ç…§å¯¹åº”æ¨¡å‹readmeå‡†å¤‡å¥½æ•°æ®ï¼Œä¿®æ”¹æ¨¡å‹å¯¹åº”çš„åœ°å€
#å¦‚è¿è¡Œ"bert:pytorch_1.8:A100:1:8:1": "/raid/home_datasets_ckpt/bert/train/"ï¼Œéœ€è¦æŠŠ:åé¢çš„è·¯å¾„æ›¿æ¢ä¸ºæœ¬åœ°è·¯å¾„
```

4. å¯åŠ¨æµ‹è¯•

```Bash
sudo python inference/run.py
```

- æ›´å¤šè®­ç»ƒ/æ¨ç†è¯´æ˜è§[è®­ç»ƒæ–‡æ¡£](https://github.com/FlagOpen/FlagPerf/blob/main/training/README.md)å’Œ[æ¨ç†æ–‡æ¡£](https://github.com/FlagOpen/FlagPerf/blob/main/docs/dev/inference-case-doc.md)

## å‚ä¸å…±å»ºFlagPerf


> å¼€å‘è€…æ•™ç¨‹ï¼šæ›´å¤šæ“ä½œæ•™ç¨‹è§ [docs-zh](./docs-zh) 

ä¸ºäº†æ›´ç›´è§‚çš„å±•ç¤ºå‚å•†å‚ä¸å…±å»ºçš„å®é™…å·¥ä½œé‡ï¼Œä¸‹é¢ç»™å‡º6ä¸ªå·²ç»åˆå¹¶è¿›FlagPerfï¼Œé¢å‘ä¸åŒç‰¹å¾å‚å•†çš„Pull Requestã€‚

1. æ¨¡å‹è®­ç»ƒé€‚é…é€‚é…

    - **ç¬¬ä¸€æ¬¡å‚ä¸è®­ç»ƒ**é€‚é…å·¥ä½œçš„å†…å®¹è¾ƒå¤šã€‚é™¤äº†é€‚é…æ¨¡å‹caseå¤–ï¼Œè¿˜éœ€è¦é€‚é…å‚å•†çš„dockerfileã€monitorç­‰ï¼Œå¦‚[#246](https://github.com/FlagOpen/FlagPerf/pull/246)
    - **åç»­å‚ä¸è®­ç»ƒ**é€‚é…å·¥ä½œé‡è¾ƒå°ï¼š
        - å¦‚å‚å•†ä»¥**cudaå…¼å®¹**è·¯çº¿è®¾è®¡è½¯ç¡¬ä»¶ï¼Œå…¸å‹é€‚é…case [#170](https://github.com/FlagOpen/FlagPerf/pull/170)
        - å¦‚å‚å•†**ä¸å…¼å®¹cuda**ï¼Œåˆ™éœ€è¦é¢å¤–ä¿®æ”¹åç«¯é€šä¿¡æ–¹æ¡ˆç­‰ç­‰ï¼Œå…¸å‹é€‚é…case [#288](https://github.com/FlagOpen/FlagPerf/pull/288)ã€‚å½“caseè¾ƒå¤æ‚æ—¶ï¼Œå¯èƒ½éœ€è¦é‡å†™éƒ¨åˆ†è®¡ç®—æ–¹å¼ã€åŠç²¾åº¦æ¥å£ç­‰ï¼Œå¦‚[#158](https://github.com/FlagOpen/FlagPerf/pull/158)ã€‚

2. æ¨¡å‹æ¨ç†é€‚é…

  - **ç¬¬ä¸€æ¬¡å‚ä¸æ¨ç†**é€‚é…çš„å·¥ä½œå†…å®¹è¾ƒå¤šã€‚é™¤äº†é€‚é…caseå¤–ï¼Œè¿˜åŒ…æ‹¬å‚å•†çš„dockerfileã€ç¼–è¯‘å™¨å®ç°æ–¹å¼ã€monitorç­‰ï¼Œå¦‚ [#256](https://github.com/FlagOpen/FlagPerf/pull/256)
  - **åç»­å‚ä¸æ¨ç†**é€‚é…æ—¶ï¼Œé€šå¸¸ä¸éœ€è¦é€‚é…å·¥ä½œé‡ã€ä»…éœ€è¿è¡Œè½¯ä»¶å®Œæˆæµ‹è¯•ã€‚å¦‚ [#227](https://github.com/FlagOpen/FlagPerf/pull/227)

## FlagPerfåˆä½œä¼™ä¼´


![cooperation](assets/imgs/logo1123.png)

## è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäºApache 2.0 licenseã€‚ 
<br>æœ¬é¡¹ç›®çš„ä»£ç æ¥æºäºä¸åŒçš„ä»£ç ä»“åº“ï¼Œå…³äºå„æ¨¡å‹æµ‹è¯•Caseçš„æƒ…å†µï¼Œè¯·å‚è€ƒå„æ¨¡å‹æµ‹è¯•Caseç›®å½•çš„æ–‡æ¡£ã€‚

## è”ç³»æˆ‘ä»¬

å¦‚æœ‰ç–‘é—®ï¼Œå¯ä»¥å‘é€é‚®ä»¶è‡³flagperf@baai.ac.cnï¼Œæˆ–åœ¨[issue](https://github.com/FlagOpen/FlagPerf/issues)ä¸­è¯´æ˜æƒ…å†µ

